 \documentclass[encoding=utf8,british]{tumphthesis}
% \documentclass[pstricks,siunitx,addfonts,theorem,font=palatino,british]{tumphthesis}
% für Dissertation:
% \documentclass[encoding=utf8,british,dissertation,font=helvet]{tumphthesis}

% Das folgende Paket dient lediglich dazu, den Blindtext "Lorem ipsum ..."
% auszugeben und kann in einer echten Abschlussarbeit natürlich weggelassen 
% (oder auskommentiert) werden.
\usepackage{lipsum}

% Die Metadaten der Abschlussarbeit (Bachelor oder Master) werden auf dem 
% Deckblatt gedruckt und in dem PDF eingetragen.
\subject{Quantum Science and Technology Master's thesis}
\title{Quantum tensor networks for sampling}
%\subtitle{\foreignlanguage{british}{Title in English}}
\author{Simon Botond}
\date{\today}
%\cooperators{Max-Planck-Institut für Physik}

% Alternativ Dissertation:
%\title{Titel der Dissertation}
%\subtitle{\foreignlanguage{british}{Title in English}}
%\author{Sheldon Cooper}
%\date{31.~Juli 2011}
%\DepartmentOrIRC{Fakultät für Physik}
%\DrDegree{der Naturwissenschaften}
%\Vorsitz{Prof. Dr. Patricia Kabelschacht}
%\ErstPrueferin{Prof. Dr. Erwin Brezensaltzer}
%\ZweitPrueferin{PD Dr. habil. Heidelinde Musterfrau}
%\DrittPrueferin{Prof. Dr. }

% Auf der Rückseite des Deckblatts können Themensteller, Zweitgutachter 
% und Tag der mündlichen Prüfung vermerkt werden.
\lowertitleback{Erstgutachter (Themensteller): Dr. Jeanette Miriam-Lorentz\\
Zweitgutachter: Alex Goessmann}

% Die Bibliographie wirde über BibLaTeX mit Biber-Backend erstellt. 
% Sofern vorhanden, wird eine bib-Datei \jobname.bib (i.d.R. also der 
% gleiche Name wie die Hauptdatei nur bib-Endung) automatisch eingebunden.
% Heißt sie anders oder müssen weitere Quelldateien eingebunden werden, so 
% dient dazu der folgende Befehl (auch z.B. bei Verwendung von ShareLaTeX 
% erforderlich, da hier \jobname auf output gesetzt wird)
% \addbibresource{thesis.bib}

\begin{document}
% Ist die Arbeit auf Englisch verfasst, hier die Sprache umschalten. 
% Die Sprache muss als Klassenoption angegeben sein.
\selectlanguage{british}

\frontmatter
\maketitle

%\newpage

\mainmatter

\chapter*{Abstract}

Efficient sampling from complex probability distributions is a cornerstone of modern machine learning, enabling 
critical tasks such as probabilistic inference, generative modeling, and optimization. Classical sampling methods, 
including brute force or Markov Chain Monte Carlo methods, face prohibitive computational challenges as the 
dimensionality and logical complexity of target distributions grow. This thesis investigates the potential of 
quantum computing—specifically amplitude amplification algorithms—to overcome these limitations and deliver practical 
advantages for machine learning sampling tasks. Through a systematic methodology, logical connectives and graphical 
models are mapped to quantum circuits via the Tensor network decomposition of their exponential family distributions.
Simulations on up to $\approx 30$ qubits (variables and ancillas together) demonstrate the scalability of quantum amplitude amplification, while 
comparative analyses with classical baselines quantify speedup and a possible turnover point. The results reveal that 
quantum-assisted sampling achieves quadratic speedups for structured distributions, though hardware constraints and 
noise pose significant challenges. This work bridges quantum computing and machine learning by formalizing a 
framework for integrating quantum sampling into classical workflows, offering a pathway toward scalable probabilistic 
inference in AI systems.

\tableofcontents

\chapter{Introduction}

    \section*{Motivation and Background}
    \label{sect:Intro_mot_back}
        Sampling is an essential part of a machine learning algorithm. Whenever the program makes a 
        decision, handles predictions with uncertainty or gives solutions to a problem, it is a form of sampling 
        from a vast set of information based on past data and / or knowledge.
        Sampling therefore is fundamental in enabling probabilistic inference, uncertainty quantification, 
        and optimization in high-dimensional spaces. As models are increasingly sophisticated — incorporating rich logical structures 
        or large graphical dependencies — their complexity escalates and the computational demands of sampling algorithms 
        grow exponentially. Classical methods, while foundational, struggle to balance accuracy and efficiency in 
        high-dimensional or multimodal distributions. Quantum computing, with its inherent parallelism and interference 
        capabilities suggest that quantum algorithms could offer speedups for certain sampling 
        tasks, motivating a systematic investigation of their applicability and impact for machine learning~\cite{Mansky_2023}\cite{PubMed}. 
        This thesis explores the intersection of quantum computing and machine learning, focusing on how quantum 
        amplitude amplification can revolutionize sampling tasks in artificial intelligence.
    The following sections elaborate on the motivation, challenges, opportunities, objectives, and structure 
    of this work.

    \section*{Classical Sampling in Machine Learning: Challenges}
    \label{sect:Intro_Challanges}
        Classical sampling algorithms, such as brute-force sampling, rejection sampling, and Markov Chain Monte Carlo (MCMC) methods;
        face significant barriers in contemporary machine learning applications. High-dimensional distributions with 
        complex dependencies or sharp peaks result in slow mixing times, autocorrelation, and exponential resource 
        scaling. For example, sampling from Bayesian networks with hundreds of variables or training deep generative 
        models with multimodal posteriors often becomes computationally intractable~\cite{Suzuki_2022}. These challenges are exacerbated 
        by the `curse of dimensionality,' where the volume of the sampling space grows exponentially with the number 
        of variables. Even state-of-the-art methods like Hamiltonian Monte Carlo or variational inference require 
        trade-offs between approximation accuracy and computational cost, leaving room for fundamentally new 
        computational paradigms.

    \section*{Quantum Computing: Opportunities for Sampling}
    \label{sect:Intro_Opportunities}
        Quantum computing introduces novel strategies for sampling through principles such as superposition or 
        entanglement, enabling new computational paradigms for sampling.  
        Recent 
        research demonstrates that quantum algorithms can sample from complex distributions with fewer resources than 
        classical counterparts, particularly for problems with combinatorial structure or where classical simulation 
        is intractable or even computationally prohibitive~\cite{larose2024briefhistoryquantumvs}\cite{Wilson_2021}. Quantum amplitude amplification, a 
        generalization of Grover's search algorithm~\cite{Grover} enables quadratic speedups in identifying `good' 
        solution states, and with repeated application also amplifying them within unstructured search spaces. Thus by encoding probability distributions into quantum 
        states, this technique can efficiently sample from distributions that would be unfeasible for classical systems.
        Also, recent advances in quantum hardware, such as improved gate fidelities and coherence times suggest that 
        near-term devices may soon support practical implementations~\cite{IQM2024}. However, practical deployment depends on 
        circuit depth, noise resilience, hardware capabilities and seamless integration with classical machine 
        learning workflows, posing as critical challenges for quantum procedures to effectively overcome classical ones~\cite{Moody2025}\cite{SpinQ2025}.

    \section*{Thesis Objectives and Contributions}
    \label{sect:Intro_Objectives}
        This thesis aims to bridge the gap between theory and practice in quantum-assisted sampling for machine 
        learning, through the following contributions:    
        \begin{itemize}
            \item A systematic methodology for mapping logical connectives and graphical models to tensor networks
            formalization of exponential family distributions, enabling their representation as quantum circuits.
            \item A framework for integrating quantum sampling into the TNREASON library, enabling hybrid 
            quantum-classical inference for nested probabilistic models.
            \item Implementation and simulation of amplification-based quantum sampling routines on said mapped distributions.
            \item Compare quantum sampling with classical baselines, quantifying speed, accuracy, and scalability,
            and analyzing theoretical speedup over classical counterparts.
            \item Resource estimation studies for near-term quantum hardware, assessing the procedure's 
            empirical performance, providing actionable insights for algorithm deployment on NISQ platforms.
        \end{itemize}

    \section*{Thesis Structure}
    \label{sect:Intro_Structure}
        The remainder of this work is organized as follows:
        \begin{itemize}
            \item \textbf{Chapter 2} reviews foundational concepts in logical connectives, graphical models, and 
            exponential familiy distributions, with their tensor network representations.
            \item \textbf{Chapter 3} introduces quantum computing, defining quantum gates and measurements, tensor network mapping,
            then turning to quantum amplitude amplification.
            \item \textbf{Chapter 4} destribes my implementation of mapping a one dimensional logical formula to a quantum circuit, 
            and performing the previously defined quantum measurement and amplitude amplification.
            \item \textbf{Chapter 5} benchmarks quantum sampling against classical methods, with analyzing scalability, and 
            comparing theoretical speedups, and discusses practical deployment constraints.
            \item \textbf{Chapter 6} explores broader implications, limitations, and future research directions, finally 
            concludes with a synthesis of contributions and an outlook on quantum sampling in AI.
        \end{itemize}
        \textcolor{red}{Benedikt: ML -> sampling -> procedure}
        \textcolor{red}{what is sampling, and how it is related to ML/AI? add definitions for data/knowledge etc.}
\chapter{Foundations}

    \section{Logical Connectives and Probabilistic Models}
    \label{sect:Foundations_modelling}
        Artificial Intelligence (AI) and Machine Learning (ML) inherently involve sampling problems during training, learning, and optimization. 
        Data can be represented through different models, each with distinct advantages. Probabilistic graphical models such as Bayesian 
        networks and Markov Random Fields, have become dominant in real-world applications. They can handle uncertainty and explicitly
        represent variable dependencies via graph structures~\cite{NumberAnalytics2025}. In contrast, logical approaches represent data as logical 
        propositions and use inference to draw deterministic conclusions, although recent developments now allow for probabilistic reasoning and sampling as well.
        \\
        With extending the practical usage of logics, the field of Statistical Relational AI bridges the gap between representations, unifying logical relations with statistical 
        models to handle uncertainty systematically. This synthesis, often termed \textit{neuro-symbolic AI}~\cite{Toward_a_broad_AI}, leverages the 
        structured reasoning of logic and the uncertainty modeling of graphical models. For instance, logical rules can be embedded as soft constraints in probabilistic 
        frameworks, enabling inference that balances symbolic precision with statistical robustness.
        \\
        Both probabilistic and logical systems can be described by a set of properties, each called a categorical variable. A so-called \textit{Atomic representation}
        of a system is described by the categorical variables $X$ taking values $x$ from a finite set:
        \begin{equation*}
            [i] := \{0,1,2,\dots, i-1\} 
        \end{equation*} 
        with cardinality $i$. We will call the variables by upper-case letters and the values by lower case letters, $X_j$ and $x_j$ respectively, with their indices serving as the connection.
        A \textit{Factored representation} of the same system is the set of categorical variables $X_j$ where $j \in [d]$, taking values in $i_j$. Thus system states are assignments to a set of variables. 
        Global properties (e.g., overall probability or formula satisfaction) are built up as combinations of local factors or constraints involving subsets 
        of these variables. 
        A small example is in order so the various concepts and representation techniques are understandable and remain easy to follow throughout the section.

        \begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black]
        
        A Bernoulli distribution is the discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $q = 1-p$.
        Now consider binary variables $X_1, \dots X_n$, each representing a logical or probabilistic variable, with values $x_1, \dots x_n$, with all $x_i \in \{0, 1\}$. Suppose these variables are jointly 
        independent Bernoulli random variables, with probabilities $p_1, \dots, p_n$ respectively. The joint distribution can be factorized as:
        \begin{equation*}
            P(X_i = x_i \; | \; i \in \{1, \dots, n\}) = P_1(x_1)P_2(x_2)\dots P_n(x_n)
        \end{equation*}
        with each $P_i$ being:
        \begin{equation*}
            P_i(x_i) = p_i^{x_i}(1-p_i)^{1-x_i}
        \end{equation*}
        thus:
        \begin{equation*}
            P(x_1, x_2, \dots , x_n) = \prod_{i = 1}^n p_i^{x_i}(1-p_i)^{1-x_i}
        \end{equation*}
        Hence the joint distibution assumes a factored structure.
        \end{tcolorbox}

        This structure admits a natural tensor representation, where variables correspond to tensor indices and their assignments to tensor 
        entries. For example, a probability distribution over binary variables $X_1, \ldots, X_n$ can be encoded as a rank-$n$ tensor 
        $\mathcal{T}[X_1, \ldots, X_n]$, with each entry storing the probability of a specific joint assignment. Similarly, logical formulas map to 
        tensors via one-hot encodings, where entries indicate formula satisfaction (1, TRUE) or violation (0, FALSE) of the given single logical connectives.
        \\
        Tensor methods provide a unifying formalism for reasoning algorithms across probabilistic and logical frameworks. Marginalization corresponds to tensor contraction, 
        while logical inference reduces to multilinear operations on formula-encoded tensors. This shared mathematical foundation enables seamless integration of probabilistic 
        graphical models and symbolic logic, paving the way for scalable quantum sampling algorithms that exploit tensor network decompositions.
        \begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black]
        In the case of our example, marginalization would mean contraction over one of the factors of the joint distribution, for example on the $k$-th index ($k \in \{1, 2, \dots, n\}$):
        \begin{equation*}
            P(x_1, x_2, \dots , x_n) = \prod_{i = 1}^{n-1} p_i^{x_i}(1-p_i)^{1-x_i} \sum_{x_k = 0}^1 p_k^{x_k}(1-p_k)^{1-x_k}
        \end{equation*}
        which for a Bernoulli distibution will result in:
        \begin{align*}
            \sum_{x_k = 0}^1 p_k^{x_k}(1-p_k)^{1-x_k} &= 1\\
            \intertext{therefore:}
            P(x_1, x_2, \dots , x_n) &= \prod_{i = 1}^{n-1} p_i^{x_i}(1-p_i)^{1-x_i}
        \end{align*}
        which is the marginal distribution for a subset of variables.
        
        \end{tcolorbox} 


        \subsection{Propositional Logic in AI}
        \label{subsect:Foundations_modelling_proplog}
            Propositional logic is a fundamental tool in artificial intelligence for representing and manipulating knowledge in a formal, symbolic manner. 
            In this framework, knowledge is encoded as a set of atomic propositions, each of which can be either true or false. These atomic propositions 
            can be combined using logical connectives such as AND ($\land$), OR ($\lor$), XOR ($\oplus$), NOT ($\neg$), IMPLIES ($\rightarrow$) and BIJECTION ($\leftrightarrow$) to form more complex 
            statements or formulas, the symbolization of which is:
            \begin{equation}
                [[A \land B] \rightarrow C] = A \; and \; B \; implies \; C,
                \label{eq:Logical_conn_ex}
            \end{equation}
            meaning that our premises are:
            \begin{itemize}
                \item \textbf{Formula:} $[[A \land B] \rightarrow C]$ 
                \item \textbf{Premise 1:} $X \rightarrow C$
                \item \textbf{Premise 2:} $[A \land B] = X$, where $X$ serves as an intermediate variable
                \item \textbf{Conclusion:} $C$
            \end{itemize}
            so that to have the formula true, we need the premises -- defined by atomic (indivisible) statements -- to be true.
            \\
            Logical reasoning in AI often involves determining the satisfiability of a set of formulas, performing inference (e.g., deducing new facts from 
            known ones), and checking entailment between statements. For example, given a knowledge base consisting of rules and facts, an AI system can 
            use logical inference mechanisms such as Modus Ponens or Resolution to derive new conclusions. This symbolic approach underpins many classical 
            AI systems, including expert systems, rule-based engines, and automated theorem provers.

            Importantly, logical connectives can be mapped to indicator functions or binary variables, which facilitates their integration into probabilistic 
            models. For instance, the truth value of a conjunction of variables can be represented as the product of their indicator variables. This mapping 
            provides a bridge between symbolic logic and statistical modeling, enabling hybrid approaches that combine the strengths of both paradigms.

        \subsection{Graphical Models: Bayesian and Markov Networks}
        \label{subsect:Foundations_modelling_Bay&Mar}
            Probabilistic graphical models (PGMs) provide a powerful framework for representing the conditional dependencies among random 
            variables in complex systems. There are two principal types of graphical models: Bayesian networks and Markov random fields 
            (also known as Markov networks).

            \textbf{Bayesian networks} are directed acyclic graphs (DAGs) in which each node represents a random variable, and each directed 
            edge encodes a conditional dependency between variables. The structure of the graph captures the causal or informational relationships 
            among the variables: a directed edge from node $X_j$ to node $X_i$ indicates that $X_j$ is a direct parent (or cause) of $X_i$.
            The joint probability distribution over all variables in a Bayesian network factorizes according to the graph structure:
            \begin{equation}
                P(X_1, \ldots, X_n) = \prod_{i=1}^n P(X_i \mid \text{Pa}(X_i))
            \end{equation}
            where $\text{Pa}(X_i)$ denotes the set of parent nodes of $X_i$. This factorization reflects the Markov property: each variable is 
            conditionally independent of its non-descendants given its parents. 
            This structure enables efficient inference and learning, especially when the graph is sparse.

            \textbf{Markov random fields} are undirected graphs where nodes represent random variables and edges encode direct, symmetric 
            interactions. The joint distribution is expressed as a product of potential functions over cliques (fully connected subsets of 
            nodes):
            \begin{equation}
                P(X_1, \ldots, X_n) = \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_C(X_C)
            \end{equation}
            where $\mathcal{C}$ is the set of cliques, each $\psi_C$ non-negative potential function over the variables in clique $C \in \mathcal{C}$, and $Z$ is the partition function 
            ensuring normalization. \textit{MRFs} also can be defined through a partition function and an energy function:
            \begin{equation}
                P(X) = \frac{1}{Z} \exp \left( \sum_i w_i \phi_i (X) \right)
                \label{eq:MLN_energy}
            \end{equation}
            where $w_i$ are weights and $\phi_i(X)$ are feature functions (often indicator functions of certain configurations).
            This energy based representation perfectly overlaps with the canonical exponential family form, where the exponent is a linear combination of sufficient statistics.
            Each potential function $\psi_C(X_C)$ can be written as an exponential of a sum of weighted features:
            \begin{equation*}
                \psi_C(X_C) = \exp \left( \sum_{i \in F_C} w_i \phi_i (X_C) \right)
            \end{equation*}
            where $F_C$ indexes the features associated with clique $C$. Substituting this into the product over cliques:
            \begin{equation*}
                \prod_{C \in \mathcal{C}} \psi_C(X_C) = \exp \left( \sum_{C \in \mathcal{C}} \sum_{i \in F_C} w_i \phi_i (X_C) \right) = \exp \left( \sum_i w_i \phi_i (X) \right)
            \end{equation*}
            since each feature $\phi_i$ is associated with a clique.
            \\
            Thus, the product-of-potentials form and the energy-based form are two views of the same distribution: 
            the first emphasizes local factors, and the second emphasizes global energy or sufficient statistics.
            \\
            The partition function can be defined as:
            \begin{equation*}
                Z = \sum_X \prod_{C \in \mathcal{C}} \psi_C(X_C) = \sum_{X} \exp \left( \sum_i w_i \phi_i (X) \right) 
            \end{equation*}
            thus ensuring normalization.
            \\
            Both Bayesian networks and Markov networks can represent high-dimensional distributions compactly, capturing complex dependencies 
            among variables. Many logical knowledge bases and rule systems can be embedded within graphical models, providing a bridge between 
            symbolic and probabilistic reasoning. For example, logical rules can be encoded as hard or soft constraints in the potentials of a 
            Markov network or as conditional probability tables in a Bayesian network.
            
            \textbf{Example:} To showcase the differences between the two a small example is in order. Imagine a street, currently dry.
            Close to it, a well-kept garden, which has a sprinkler. The sky is kind of cloudy at the moment. Will the street be wet in the near future?
            The dependencies of course lie between the given nodes, as the `wet street' variable is dependent on the fact, whether it is raining, or if the 
            sprinkler is being used by the gardener, which of course also (somewhat) depends on the state of the weather. So our variables are:
            \begin{itemize}
                \item \textbf{A} - \textit{Rain:} True/False
                \item \textbf{B} - \textit{Sprinkler on:} True/False
                \item \textbf{C} - \textit{Wet street:} True/False
            \end{itemize}

            Which admits to a factorization using a Bayesian network:
            \begin{equation*}
                P(A, B, C) = P(A)P(B|A)P(C|A, B)
            \end{equation*}
            and with a Markov logic network:
            \begin{equation*}
                P(A, B, C) = \frac{1}{Z} \psi_{CA}P(C, A) \psi_{CB}P(C, B) \psi_{BA}P(B, A)
            \end{equation*}
            where each $\psi$ is a potential function (not necessarily normalized).

    \section{Exponential Family Distributions}
    \label{sect:Foundations_ExpFam}
        \subsection{Definition and Properties}
        \label{subsect:Foundations_ExpFam_Definition}
            The exponential family is a broad class of probability distributions that share a common functional (energy based) form, making them 
            particularly amenable to statistical inference and learning~\cite{NOW-exp}\cite{Geiger-exp}. A big group of distirbutions can be represented
            as exponential families, such as Bernoulli, Normal, Binomial, etc.; just like previously mentioned logical formulas~\ref{subsect:Foundations_modelling_proplog} and 
            graphical models~\ref{subsect:Foundations_modelling_Bay&Mar}.

            \textbf{Canonical parameterization}
            
            A probability distribution $p(x\,|\,\theta)$ belongs to the exponential family if it can be parametrized as:
            \begin{equation}
                p(x) = \exp \{ \langle \theta , \phi(x) \rangle - A(\theta) \}
                \label{eq:expfam_def}
            \end{equation}
            where $\theta$ is the vector of canonical (or exponential) parameters which weight the features at a given state to calculate the
            respective probability; $\phi(x)$ is the sufficient statistics, a function of the data that encapsulates all the information relevant 
            for inference about the canonical parameter parameter, usually a vector containing the features themselves; and $A(\theta)$ is the cumulant function, 
            (also called the log-partition function as it can be reshaped s.t.: $Z = exp\{- A(\theta)\}$) ensuring normalization, as per:
            \begin{equation*}
                A(\theta) = \log \int_{\chi^m} \exp \langle \theta , \phi(x) \rangle \nu dx
            \end{equation*}
            where we presume that the integral is finite, so this defnition ensures that $p(x)$ is properly normalized.
            Thus the canonical parameterization of an exponential family is, as the name suggests, unique, characterized by the canonical parameters.
            \textbf{Mean parameterization}

            In addition to the canonical parameterization by canonical parameters and sufficient statistics, every exponential family admits 
            a dual, alternative, so-called mean parameterization. The mean parameters $\mu_{\alpha}$ are
            associated with the sufficient statistic $\phi_{\alpha}$ as it is defined as the expected values of the sufficient statistics under the distribution:
            \begin{equation}
                \mu_{\alpha} = \mathbb{E}_{p}[\phi_{\alpha}(x)],
                \label{eq:expfam_def_mean}
            \end{equation}
            thus a mean parameter can be defined for each sufficient statistic, creating a set of all realizable mean parameters, known as the 
            \textit{marginal polytope} or \textit{mean parameter space}:
            \begin{equation}
                \mathcal{M} := \{ \mu \in \mathbb{R}^d \: | \: \exists \: p \; s.t. \; \mathbb{E}[\phi_{\alpha}(X)] = \mu_\alpha\}
            \end{equation}

            A fundamental property of exponential families is that the mapping from canonical parameters $\theta$ to mean parameters $\mu$ 
            is given by the gradient of the log-partition function:
            \begin{equation}
                \mu = \nabla_\theta A(\theta),
            \end{equation}
            When switching from canonical to mean parameterization, the natural parameter $\theta$ is replaced with the mean parameter 
            $\mu$, using the gradient of the log-partition function to map between them. The sufficient statistics $\phi(x)$ remain unchanged, 
            but the way the distribution is parameterized and interpreted changes fundamentally:
            \begin{equation}
                p(x) = \exp \{ \langle \theta (\mu) , \phi(x) \rangle - A(\theta(\mu)) \}
                \label{eq:expfam_def_mean}
            \end{equation}
            and, for minimal exponential families, this mapping is bijective between the interior of the mean parameter space and the 
            canonical parameter space. This duality underlies variational inference, maximum likelihood estimation, and moment-matching procedures.
            In practice, mean parameters often correspond to marginal 
            probabilities (e.g., node and edge marginals in graphical models), and many inference algorithms (such as mean field and 
            variational methods) operate directly in the mean parameter space. For example, marginalization can be understood as transfroming from one parametrization to the other.
            The comparison of the parametrizations can be seen in the table below. 

            \begin{table}[h!]
            \centering
            \renewcommand{\arraystretch}{1.2}
            \begin{tabular}{|l|l|l|}
            \hline
            & \textbf{Canonical (Natural) Param.} & \textbf{Mean Param.} \\
            \hline
            \textbf{Parameter vector} & $\theta$ & $\mu = \mathbb{E}_\theta[\phi(x)]$ \\
            \textbf{Interpretation} & Encodes the effect of sufficient statistics & Expected sufficient statistics (marginals) \\
            \textbf{Normalization} & $A(\theta) = \log \int e^{\langle \theta, \phi(x)\rangle}h(x)dx$ & $A^*(\mu)$ is the conjugate dual \\
            \textbf{Transformation} & $\mu = \nabla_\theta A(\theta)$ & $\theta = (\nabla_\theta A)^{-1}(\mu)$\\
            \textbf{Inference role} & Convenient for parametric modeling, optimization & Used by mean field, variational inference, direct estimation\\
            \textbf{Examples} & Log-odds, log-potentials, energy parameters & Marginal probabilities, moments\\
            \hline
            \end{tabular}
            \caption{Comparison of canonical and mean parameterizations in minimal exponential families~\cite{WainwrightJordan2008}.}
            \end{table}

            Key properties of exponential family distributions include:
            
            \begin{itemize}
                \item \textbf{Sufficient statistics}: The function $\phi(x)$ captures all information about the data relevant to 
                parameter estimation.
                \item \textbf{Conjugacy}: Many exponential family distributions admit conjugate priors, simplifying Bayesian 
                inference.
                \item \textbf{Tractable moments}: Moments and cumulants of the distribution can be computed as derivatives of 
                $A(\theta)$.
                \item \textbf{Generalization}: Many common distributions, such as the Bernoulli, multinomial, normal, and Poisson, 
                and Markov Logic Networks as well are members of the exponential family.
            \end{itemize}

            In the context of AI and machine learning, exponential familiy representations can provide a general, 
            unified framework that encompasses many commonly used distributions, such as probabilistic graphical models and logic-based systems. 
            By choosing appropriate sufficient statistics (such as indicator functions for logical formulas), 
            one can encode logical constraints and probabilistic dependencies within a single, tractable mathematical formalism.

        \subsection{MLNs as exponential families}
        \label{subsect:Foundations_ExpFam_MLN}
            Many logical knowledge bases, Markov logic networks (MLNs), and rule-based systems can be naturally expressed within the 
            exponential family framework. In MLNs, for example, each logical formula $\phi_i$ is associated with a weight $\theta_i$, 
            and the probability of a world (i.e., a complete assignment of truth values) is given by:
            \begin{equation}
                P(X) \propto \exp\left( \sum_i \theta_i \phi_i(X) \right)
            \end{equation}
            as seen in eq.~\ref{eq:MLN_energy}. Here, $\phi_i(X)$ is an indicator function that evaluates to 1 if the formula is satisfied by $X$, and 0 otherwise. The 
            weights $\theta_i$ determine the strength of each logical constraint: large positive values enforce hard constraints, 
            while smaller values allow for soft, probabilistic reasoning.

            This representation allows for the seamless integration of symbolic logic and statistical learning. Logical connectives 
            can be encoded as sufficient statistics in the exponential family, and their weights can be learned from data. As the 
            weights become large, the model approaches a purely logical system; as they decrease, the model becomes more probabilistic, 
            capturing uncertainty and noise in the data.

            \begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black]
                After showing how the exponential family picture works and how an easy example can be presented via Markov Logic Networks, it would be time to 
                have our example distribution of jointly independent Bernoulli variables mapped to an exponential family as well. 
                \\
                The distibution is:
                \begin{equation*}
                    P(x_1, x_2, \dots , x_n) = \prod_{i = 1}^n p_i^{x_i}(1-p_i)^{1-x_i}
                \end{equation*}
                where as we work with independent variables, the sufficient statistics and canonical variables are straightforward for the 
                canonically parametrized exponential family:
                \begin{equation*}
                    P(x_1, x_2, \dots , x_n) = \exp{\sum_{i=1}^n \theta_i x_i - A(\theta)}
                \end{equation*}
                \begin{itemize}
                    \item \textbf{Sufficient statistics:} are the counts of successes in our case, esentially the grounding of our formulas, i.e. when are we given a 
                    $1$ as a result: $\phi(x_1, x_2, \dots , x_n) = (x_1, x_2, \dots , x_n)$,
                    \item \textbf{Canonical parameters:} they are ought to be representing the \textit{probabilities} of each distibution resulting in $1$ or $0$:
                    $\theta_i = log \left(\frac{p_i}{1 - p_i}\right)$.
                    \item \textbf{Log-partition function:} should be normalizing the distribution, thus it should be $A(\theta) = \sum_{i=1}^n log( 1 + e^{\theta_i})$
                \end{itemize}
                If we insert all of the above into the definition of the exponential family, we will get the Bernoulli distribution back, as per:
                \begin{equation*}
                    P(x_1, x_2, \dots , x_n) &= \exp\left\{\sum_{i=1}^n \left[ log \left(\frac{p_i}{1 - p_i}\right) \cdot x_i \right] - \left[ log( 1 + e^{log \left(\frac{p_i}{1 - p_i}\right)})\right]\right\}
                \end{equation*}
                With the first parantheses within the sum being the inner product of the sufficient statistics and canonical parameters, and the second the log-partition function. 
                Let me open up the first parantheses to have:
                \begin{align*}
                    [1.] &= \exp\left\{\sum_{i=1}^n \left[ log \left(\frac{p_i}{1 - p_i}\right) \cdot x_i \right]\right\} \\
                    \intertext{as it is the exponential of a sum, we can treat it as multiplying n-many exponentials together as such:}
                    [1.] &= \prod_{i=1}^n \exp\left\{ \left[ log \left(\frac{p_i}{1 - p_i}\right) \cdot x_i \right]\right\} \\
                    \intertext{and for each we can utilize that $e^{a log(b)} = b^a$:}
                    [1.] &= \prod_{i=1}^n \left(\frac{p_i}{1 - p_i}\right)^x_i \\
                \end{align*}
                And the second parantheses:
                \begin{align*}
                    [2.] &= \exp\left\{\sum_{i=1}^n - \left[log( 1 + e^{log \left(\frac{p_i}{1 - p_i}\right)})\right]\right\} \\
                    \intertext{we are dealing with the exponential of a sum again:}
                    [2.] &= \prod_{i=1}^n \exp\left\{ - \left[log( 1 + e^{log \left(\frac{p_i}{1 - p_i}\right)})\right]\right\} \\
                    [2.] &= \prod_{i=1}^n  \frac{1}{1 + \left(\frac{p_i}{1 - p_i}\right)} \\
                \end{align*}
                Thus we have:
                \begin{align*}
                    P(x_1, x_2, \dots , x_n) &= \prod_{i=1}^n \frac{\left(\frac{p_i}{1 - p_i}\right)^x_i}{1 + \left(\frac{p_i}{1 - p_i}\right)} \\
                    P(x_1, x_2, \dots , x_n) &= \prod_{i=1}^n (1 - p_i) \cdot \left( \frac{p_i}{1 - p_i} \right)^{x_i} \\
                    P(x_1, x_2, \dots , x_n) &= \prod_{i=1}^n p_i^{x_i}(1-p_i)^{1 - x_i}
                \end{align*}
                which is the original form of the example distribution.
                \\
                Also another form of the exponential family distribution have been defined, which is the \textit{mean parametrization}. 
                The mapping from canonical parameters $\theta$ to mean parameters $\mu$ is given by the gradient of the log-partition function:
                \begin{equation*}
                    \mu = \nabla_\theta A(\theta),
                \end{equation*}
                with the exponential family being:
                \begin{equation}
                    P(X) \propto \exp\left( \sum_i \theta_i \phi_i(X) \right)
                \end{equation}
                Using the equation above we see that the mean parameter $\mu_i$ for a given variable $X_i$ should be:
                \begin{align*}
                    \mu_i &= \\
                    \intertext{thus the mean parameter vector:}
                    \mu &= \sum_{i = 1}^n
                \end{align*}
                \textcolor{red}{finalize the example for the exponential families, i.e add mean params.}

            \end{tcolorbox}

    \section{Tensor Networks}
    \label{sect:Foundations_TN}

        \subsection{Tensor Network Basics}
        \label{subsect:Foundations_TN_Basics}

            A \textit{tensor} is a multi-dimensional array generalizing scalars (0th order), vectors (1st order), and matrices 
            (2nd order) to arbitrary dimensions. Formally, an $n$th-order tensor $\mathcal{T} \in \mathbb{R}^{d_1 \times \cdots \times 
            d_n}$ has $n$ indices, each ranging over dimensions $d_1, \ldots, d_n$. Tensors enable compact representation of 
            high-dimensional functions, such as joint probability distributions over many variables.

            A \textit{tensor network} is a structured factorization of a high-order tensor into a collection of lower-order tensors 
            connected via contractions over shared indices. Graphically, nodes represent tensors, and edges denote contracted indices.

            Key operations in tensor networks include:
            \begin{itemize}
                \item \textbf{Contraction}: Summing over shared indices, e.g., $\mathcal{C}[i,j] = \sum_k \mathcal{A}[i,k] \mathcal{B}[k,j]$ for matrix multiplication.
                \item \textbf{Decomposition}: Approximating $\mathcal{T}$ as a network/sequence of simpler (i.e. lower dimensional) tensors  
                (like Canonical-Polyadic (CP), Matrix Product State (MPS), or tensor train (TT)).
                \item \textbf{Normalization}: Enforcing constraints (e.g., non-negativity) to represent valid probability distributions.
            \end{itemize}

            Tensor networks excel at capturing locality and hierarchy in data. For example, a tree tensor network (TTN) mirrors the 
            structure of a Bayesian network, with each node representing a conditional probability table and edges encoding variable 
            dependencies.

        \subsection{Tensor Networks for Probabilistic Models}
        \label{subsect:Foundations_TN_ProbNetw}

            The joint probability distribution encapsulated in a graphical model can be encoded as a tensor where each entry $\mathcal{T}[x_1, 
            \ldots, x_n]$ stores $P(X_1=x_1, \ldots, X_n=x_n)$. Tensor networks factorize this high-dimensional tensor into localized 
            components reflecting the model's conditional independence structure.

            \textbf{Example: Bayesian Network as a Tensor Network} \\
            Consider a Bayesian network over binary variables $X_1, X_2, X_3$ with edges $X_1 \rightarrow X_2$ and $X_2 \rightarrow X_3$. 
            The joint distribution is:
            \begin{equation}
                P(X_1, X_2, X_3) = P(X_1)P(X_2|X_1)P(X_3|X_2)
            \end{equation}
            Which factorizes to a tensor network:
            \begin{equation}
                \mathcal{T}[X_1, X_2, X_3] = \sum_{Y} \mathcal{A}[X_1]\mathcal{B}[X_1, X_2, Y]\mathcal{C}[X_2, X_3, Y]
            \end{equation}
            where $Y$ is an auxiliary index connecting the two conditional tensors 
            $\mathcal{B}[X_1, X_2, Y]$ and $\mathcal{C}[X_2, X_3, Y]$, with $\mathcal{A}[X_1]$ being the prior tensor to $X_1$. The conditional tensors 
            have been added an additional so-called bond index $Y$, which serves as a virtual bond indexing the possible channels (i.e. connection) between $X_2$ and its
            neighbours $X_1$ and $X_3$, by which the dependency can be communicated through. It is not a probabilistic variable, and its dimension is the number of possible 
            states being passed along, in our case the possible assignments to $X_2$ (so for binary variables, the dimension should be 2 as $X_2 \in \{0, 1\}$)

            \textbf{Example: Markov Random Field as a Tensor Network} \\
            Consider a Markov random field (MRF) over binary variables $X = \{X_1, X_2, X_3\}$, with direct edges between $\{X_1, X_2\}$ and $\{X_2, X_3\}$. These are the clicques contained 
            in the graph. The joint distribution is:
            \begin{equation}
                P(X) = \frac{1}{Z} \psi_{12}(X_1, X_2) \psi_{23}(X_2, X_3)
            \end{equation}
            This corresponds to a tensor network:
            \begin{equation}
                \mathcal{T}[X_1, X_2, X_3] = \sum_{Y} \psi_{12}[X_1, X_2, Y] \psi_{23}[Y, X_2, X_3]
            \end{equation}
            where $Y$ is an auxiliary index connecting the two clique tensors. This meets the Tensor Network representation of Propositional 
            Logical formulas as well, where we can define the unique Logical connectives through the $\psi_{X_n, X_m, Y}$ functions,
            with $X_n, X_m$ being the variables for the connective and the $Y$ should be the `ancilla' connecting the connectives to 
            have the Logical formula in the end.

            
            \textbf{Key Applications relevant to Machine Learning frameworks:}
            \begin{itemize}
                \item \textbf{Marginalization}: Marginalizing over variables—to compute $P(X_i)$ or any subset of marginals—involves summing (contracting) over all indices of the tensor 
                network except those corresponding to the target variable(s). In traditional representations, this would require summing over an exponential number of configurations. 
                However, when the model is represented as a tree tensor network (TTN) or similarly structured tensor network (e.g., MPS/TTN for chain/tree-like dependencies), 
                contractions can be performed locally and in sequence, resulting in an overall computational cost that scales linearly (or at most polynomially) with the number of variables $n$.
                \item \textbf{Sampling}: Sampling from the joint distribution can be realized efficiently within the tensor network framework. In Bayesian networks, this reduces to ancestral sampling: 
                start at the root nodes, sample values according to their local tensors (conditional probabilities or potentials), and propagate these values down the network, sequentially sampling 
                each variable conditioned on its parents. For a directed acyclic tensor network, this is equivalent to performing a sequence of local contractions and probabilistic selections, 
                efficiently navigating the high-dimensional sample space by leveraging the graphical structure.
                \item \textbf{Learning}: Parameter estimation, particularly maximum likelihood learning, is naturally phrased as tensor factorization in this context. The goal is to decompose 
                the observed joint distribution tensor into a product of lower-order tensors (factors), such that their contraction approximates the data tensor as closely as possible. Algorithms 
                like alternating least squares (ALS) iteratively update one tensor factor at a time, holding the others fixed, to minimize reconstruction error. This approach generalizes to learning 
                in graphical models, tensor decomposition of observed statistics, and parameter estimation in large exponential family models where full enumeration and direct optimization would be infeasible.
            \end{itemize}

	\section{Tensor Network Representation of Exponential Families}
	\label{sect:sect:Foundations_EXP2TN}
        Exponential family distributions, due to their structured parameterization and sufficient statistics, are naturally suited for 
        representation as tensor networks. This correspondence enables efficient manipulation, marginalization, and sampling—crucial for 
        high-dimensional probabilistic models and for mapping to quantum circuits.

		\subsection{Slice Tensor Decomposition}
		\label{subsect:Foundations_EXP2TN_slice}

		A central technique for representing high-order tensors arising from exponential family models is \textit{slice tensor decomposition}. 
        In this approach, a high-dimensional tensor $\mathcal{T}[X_1, \ldots, X_n]$ encoding the joint distribution is factorized into a sum 
        of lower-rank tensors (slices) along one or more modes:
		\begin{equation}
		    \mathcal{T}[X_1, \ldots, X_n] = \sum_{k=1}^r \lambda_k \, \mathcal{A}_k[X_1] \otimes \mathcal{B}_k[X_2, \ldots, X_n]
		\end{equation}
		where $\lambda_k$ are scalar coefficients, and $\mathcal{A}_k$, $\mathcal{B}_k$ are subtensors or vectors corresponding to particular 
        slices. This decomposition preserves the conditional independence structure of the underlying graphical or logical model and allows 
        for efficient storage and computation, especially when the tensor admits a low-rank structure.

		Slice decomposition is particularly powerful for models with factorizable sufficient statistics, such as those arising in Markov logic 
        networks or graphical models, where each logical formula or clique potential can be associated with a tensor slice. The resulting \
        network of slices can then be contracted (multiplied and summed over shared indices) to recover marginals or conditionals.

        \subsection{Operational Mapping: Step-by-Step Example}
        \label{subsect:StepByStep}
        To illustrate, consider a simple exponential family model over three binary variables $X_1, X_2, X_3$. The joint distribution can 
        be encoded as a rank-3 tensor $\mathcal{T}[X_1, X_2, X_3]$. Suppose the model factorizes as:
        \begin{equation}
            \mathcal{T}[X_1, X_2, X_3] = \sum_{k=1}^r \mathcal{S}_k[X_1] \cdot \mathcal{U}_k[X_2] \cdot \mathcal{V}_k[X_3]
        \end{equation}
        where each $\mathcal{S}_k, \mathcal{U}_k, \mathcal{V}_k$ is a vector (or slice) over its respective variable, and $r$ is the 
        decomposition rank. This format is equivalent to the canonical polyadic (CP) decomposition and is especially efficient when $r$ is 
        small compared to the full tensor size.

        In practice, such decompositions can be obtained via algebraic methods (e.g., alternating least squares) or by exploiting the 
        structure of the model (e.g., using the factor graph or logical formulae). The resulting tensor network can then be contracted to 
        compute marginals, conditionals, or to generate samples.


\chapter{Quantum Sampling Algorithms}
    With the previous chapter containing classical representations, decomposition techniques and methods, this chapter will provide insight 
    to quantum algorithms and methods. Basics of quantum circuits, with gates and measurements will be the starting point of getting acquainted with the notations 
    and inner workings of a quantum algorithm, with the main component of our procedure -- the amplitude amplification -- being defined just afterwards. Finishing the chapter 
    I will turn briefly to slice tensor decomposition methodology, defined in sect.~\ref{subsect:Foundations_EXP2TN_slice}, to extend the definition, thus providing a path 
    for mapping classical representations~\ref{sect:Foundations_modelling} to measurable quantum states.
    \section{Quantum Gates, Circuits, and Measurement}
    \label{sect:QSA_Basics}
    Quantum gates, circuits, and measurements together provide the operational foundation for all quantum algorithms. Gates manipulate the 
    amplitudes and phases of quantum states, circuits implement complex transformations, and measurement extracts classical information, 
    enabling quantum algorithms to outperform their classical counterparts in sampling, search, and inference~\cite{Nielsen_Chuang_2010}.

    This section summarizes the essential elements relevant for later procedures, such as amplitude amplification and the collective quantum sampling algorithm.

        \subsection{Qubit States and Quantum Registers}
        A single qubit is described by a state vector in a two-dimensional Hilbert space,
        \begin{equation*}
            \ket{a} = v_0\ket{0} + v_1\ket{1},
        \end{equation*}
        where $v_0$ and $v_1$ are complex amplitudes satisfying $|v_0|^2 + |v_1|^2 = 1$. The basis states $\ket{0}$ and $\ket{1}$ are 
        represented as column vectors:
        \begin{equation*}
            \ket{0} = \begin{bmatrix}1 \\ 0\end{bmatrix}, \quad \ket{1} = \begin{bmatrix}0 \\ 1\end{bmatrix}.
        \end{equation*}
        For multi-qubit systems, the overall state is given by the tensor product of individual qubit states. For example, a two-qubit state is
        \begin{equation*}
            \ket{\psi} = v_{00}\ket{00} + v_{01}\ket{01} + v_{10}\ket{10} + v_{11}\ket{11}.
        \end{equation*}

        \subsection{Quantum Gates: Basic Building Blocks}
        Quantum gates are unitary operations acting on one or more qubits. They generalize classical logic gates but can create 
        superposition and entanglement, enabling quantum parallelism.

        \paragraph{Single-Qubit Gates:}
        \begin{itemize}
            \item \textbf{Pauli-X (NOT) Gate:} Flips $\ket{0}$ to $\ket{1}$ and vice versa.
            \begin{equation*}
                X = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}
            \end{equation*}
            \item \textbf{Pauli-Y and Pauli-Z Gates:} $Y$ combines a bit and phase flip; $Z$ applies a phase flip to $\ket{1}$.
            \begin{equation*}
                Y = \begin{bmatrix}0 & -i \\ i & 0\end{bmatrix}, \quad Z = \begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix}
            \end{equation*}
            \item \textbf{Hadamard (H) Gate:} Creates superposition. Applied to $\ket{0}$, it yields $(\ket{0} + \ket{1})/\sqrt{2}$.
            \begin{equation*}
                H = \frac{1}{\sqrt{2}}\begin{bmatrix}1 & 1 \\ 1 & -1\end{bmatrix}
            \end{equation*}
            \item \textbf{Phase Gates:} modifies the phase of the quantum state, but the measurement probabilities stay.
            \begin{equation*}
                P(\varphi) = \begin{bmatrix} 1 & 0 \\ 0 & e^{i\varphi} \end{bmatrix}
            \end{equation*}
            \item \textbf{Rotation Gates:} $R_x(\theta)$, $R_y(\theta)$, and $R_z(\theta)$ rotate the qubit state around the respective axes by 
            angle $\theta$.
            \begin{equation*}
                R_{P_g}(\theta) = e^{-iP_g\theta/2} = \cos{\frac{\theta}{2}} \cdot 1 - i \sin{\frac{\theta}{2}} \cdot P_g
            \end{equation*}
            where $P_g$ are the possible Pauli gates (\textit{X, Y, Z}).
        \end{itemize}

        \paragraph{Multi-Qubit Gates:}
        \begin{itemize}
            \item \textbf{SWAP Gate:} Exchanges the states of two qubits.
            \begin{equation*}
               Swap = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix} 
            \end{equation*}
            \item \textbf{CNOT (Controlled-NOT) Gate:} A two-qubit gate that flips the target qubit if the control qubit is $\ket{1}$. 
            Essential for generating entanglement. 
            \begin{equation*}
                CNot = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ \end{bmatrix}
            \end{equation*}
            Also, it can be stylized as $C\hat{X}$ as the \textit{Not} operation applied to the target qubit is a \textit{Pauli-X} gate.
            \item \textbf{Toffoli (CCNOT) Gate:} A three-qubit gate; flips the third (target) qubit if both control qubits are $\ket{1}$. 
            Important for universal reversible computation and error correction.
            \begin{equation*}
                Toffoli = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
                    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ 
                    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 
                    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\ \end{bmatrix}
            \end{equation*}
        \end{itemize}
        Quantum gates are represented as unitary matrices. A gate acting on $n$ qubits is a $2^n \times 2^n$ unitary matrix.

        \subsection{Quantum Circuits}
        A quantum circuit is a sequence of quantum gates applied to qubits, typically followed by measurement. Circuits are often depicted 
        as diagrams, with qubits as horizontal lines and gates as symbols acting on these lines. For example, the circuit for creating a 
        Bell state applies a Hadamard gate to the first qubit, followed by a CNOT with the first as control and the second as target, as 
        seen in the figure below~\ref{fig:Entangling_circuit}.

        \begin{figure}[ht]
        \centering
        \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.85\linewidth]{pics/entangling.png}
        \caption{Entangling circuit}
        \label{fig:Entangling_circuit_A}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.85\linewidth]{pics/entangling_results.png}
        \caption{Result of circuit measurement}
        \label{fig:Entangling_circuit_B}
        \end{subfigure}
        \caption{Example circuit, resulting in $\hat{U}\ket{00} = C\hat{X}_{XY}\hat{H}_X\ket{00} = \frac{1}{\sqrt{2}}(\ket{00} + \ket{11})$. 
        \textcolor{magenta}{Later it will be changed up to make it fit with the overall color scheme etc, right now it is a placeholder}}
        \label{fig:Entangling_circuit}
        \end{figure}

        The action of a circuit on an initial state $\ket{\psi_0}$ is a sequence of unitary transformations:
        \[
        \ket{\psi_{\text{final}}} = U_k \cdots U_2 U_1 \ket{\psi_0},
        \]
        where each $U_i$ is a quantum gate.

        \subsection{Measurement in Quantum Mechanics}

        A measurement is the testing or manipulation of a physical system to yield a numerical result. In quantum mechanics it is when 
        a quantum state collapses to a classical outcome. In the computational basis, measuring a qubit in state $\ket{a} = v_0\ket{0} + v_1\ket{1}$ 
        yields $\ket{0}$ with probability $|v_0|^2$ and $\ket{1}$ with probability $|v_1|^2$.

        For multi-qubit systems, measurement projects the state onto one of the basis vectors $\ket{k}$, with probability $|v_k|^2$. 
        The outcome is inherently probabilistic, a fundamental departure from classical computation.

        \paragraph{Entanglement and Measurement:}
        Measurement on one qubit of an entangled pair instantaneously determines the outcome of the other, a phenomenon with no classical 
        analog. For example, measuring one qubit of the Bell state $(\ket{00} + \ket{11})/\sqrt{2}$ collapses the other qubit to the 
        same value, although causality must be preserved, hence the no communication theorem should be obeyed~\cite{No_comm_theor}.
        It will be utilized in my implementation to enable backpropagation as a learning procedure.

    \section{Amplitude Amplification: Theory and Practice}
    \label{sect:QSA_AA}
    Amplitude amplification is a quantum algorithmic technique that generalizes Grover's search~\cite{Grover}, providing a quadratic 
    speedup for the identification or sampling of `good' states in a large, unstructured search space. The power of amplitude 
    amplification lies in its ability to systematically increase the probability amplitude associated with desirable outcomes, while 
    keeping the state space normalized, making it a central primitive for quantum-enhanced sampling in machine learning.

        \subsection{Grover’s Algorithm and Generalizations}

        Grover’s algorithm is the canonical example of amplitude amplification. In the classical setting, searching for a marked item in an 
        unsorted database of size $N$ requires, on average, $O(N)$ queries, as with random selection, each and every item would have a 
        $\frac{1}{N}$ probability of finding. Grover’s quantum approach reduces this to $O(\sqrt{N})$ by exploiting quantum superposition 
        and entanglement.

        The algorithm operates in an $N$-dimensional Hilbert space $\mathcal{H}$, where each basis state $\ket{k}$ represents a possible 
        solution. A Boolean oracle function $\chi: \{0,1\}^n \rightarrow \{0,1\}$ identifies `good' states. The oracle operator $\mathbf{O}$ 
        applies a phase flip to these states, while the diffusion operator $\mathbf{D}$ amplifies their amplitudes.

        Brassard et al.~\cite{Brassard_2002} extended Grover’s idea to arbitrary initial states and general quantum algorithms, formalizing the amplitude amplification operator:
        \begin{equation*}
        \mathbf{Q} = -\mathcal{A} \mathbf{B}_0 \mathcal{A}^{-1} \mathbf{B}_\chi
        \end{equation*}
        where $\mathcal{A}$ prepares the initial state, $\mathbf{B}_\chi$ flips the phase of good states, and $\mathbf{B}_0$ flips the phase of the all-zero state. Repeated application of $\mathbf{Q}$ rotates the quantum state in the two-dimensional subspace spanned by the good and bad components, exponentially increasing the probability of measuring a good state.

        \subsection{Oracle and Diffusion Operator Design}

        The effectiveness of amplitude amplification hinges on the careful design of the oracle and diffusion operators.

        \paragraph{Oracle Operator $\mathbf{O}$:}
        The oracle is a unitary operator that marks the set of good states by flipping their phase. For a basis state $\ket{k}$, in $\mathcal{S} := \{\ket{k}\}_{k=0}^{N-1}$,
        \begin{equation*}
        \mathbf{O} \ket{k} = (-1)^{\chi(k)} \ket{k}
        \end{equation*}
        where $\chi(k) = 1$ for good states and $0$ otherwise. In practical terms, the oracle is implemented as a quantum circuit that 
        evaluates the Boolean function $\chi$ and applies a controlled-$Z$ or multi-controlled Toffoli gate to flip the phase of the target states. 
        For simple logical connectives, such as AND or OR, the circuit construction is straightforward. For more complex constraints, the circuit 
        depth increases, but the principle remains the same: the oracle must be a reversible, unitary operation that encodes the solution set 
        into phase flips.

        \paragraph{Diffusion Operator $\mathbf{D}$:}
        The diffusion operator, or “inversion about the mean,” amplifies the amplitudes of the marked states. For an initial state 
        $\ket{\psi} = \mathcal{A}\ket{0}$, the diffusion operator is
        \begin{equation*}
        \mathbf{D} = 2\ket{\psi}\bra{\psi} - \mathbb{I}
        \end{equation*}
        This operator reflects the quantum state about the initial state vector. In the special case where $\ket{\psi}$ is the uniform superposition, $\mathbf{D}$ can be implemented by Hadamard gates, a phase flip on $\ket{0}$, and another round of Hadamards. For non-uniform initial states, the diffusion operator is constructed by applying the inverse of the state preparation circuit, a phase flip on $\ket{0}$, and then re-applying the state preparation.

        \paragraph{Geometric Interpretation:}
        The interplay between the oracle and diffusion operators can be visualized as a sequence of reflections in a two-dimensional subspace. 
        With a projection operator emerging from the definition of $\mathbf{O}$;\
        \begin{equation*}
            \mathcal{P} := \sum_{\chi(k) = 1} \ket{k}\bra{k}
        \end{equation*} 
        defining the good and bad subspaces respectively:
        \begin{align*}
            \mathcal{H}_G := Im(\mathcal{P}) = span\{ \ket{k} \in \mathcal{S}_{op} \; | \;\chi(k) = 1\}
            \\
            \mathcal{H}_B := Ker(\mathcal{P}) = span\{ \ket{k} \in \mathcal{S}_{op} \; | \;\chi(k) = 0\}
        \end{align*} 
        If we adopt the convention of $\sum'$ for summation over the solution states, and $\sum''$ for summation over the non-solutions, we can define normalized states as such:
        \begin{align*}
            \ket{x_G} = \sqrt{\frac{1}{M}} \sum'_x \ket{x} 
            \\
            \ket{x_B} = \sqrt{\frac{1}{N - M}} \sum''_x \ket{x}
        \end{align*}
        where \textbf{N} is the number of states ($2^n$ for n qubits) and \textbf{M} is the number of solution states. Thus the initial state can be realized as:
        \begin{equation*}
            \ket{\psi} = \sqrt{\frac{N - M}{N}} \ket{x_B} + \sqrt{\frac{M}{N}}\ket{x_G} = \cos\left( \frac{\alpha}{2} \right) \ket{x_B} + \sin\left( \frac{\alpha}{2} \right)\ket{x_G}
        \end{equation*} 
        This is then a two-dimensional subspace spanned by the vectors $\ket{x_G}$ and $\ket{x_B}$ is stable under the action of $\mathbf{Q}$.
        The oracle reflects the state about the bad subspace, while the diffusion operator reflects about the initial state.

        The composition of these two reflections is a rotation by $\alpha$ towards the good subspace. This geometric process underlies 
        the quadratic speedup of amplitude amplification.


        \subsection{Parameter Update Interpretation}
        \label{subsect:QSA_AA_paramupdate}

        The action of the amplitude amplification operator $\mathbf{Q}$ can be interpreted as two reflections -- once over the bad states, 
        and then over the average amplitude of our states (i.e. the register itself in the two-dimensional subspace) -- which geometrically is a rotation in the 
        two-dimensional subspace of the Hilbert space spanned by the projections of the initial state onto the good and bad subspaces. The initial state can be 
        written as
        \begin{equation*}
        \ket{\psi} = \sin \left( \frac{\alpha}{2} \right)\ket{\psi_g} + \cos \left( \frac{\alpha}{2} \right)\ket{\psi_b}
        \end{equation*}
        where $\ket{\psi_g}$ and $\ket{\psi_b}$ are normalized projections onto the good and bad subspaces respectively, ultimately resulting in
        the 2 dimensional subspace, which I will refer to as $H_{\psi}$. 
        The result of said rotations in $H_{\psi}$ can be depicted as seen in figure~\ref{fig:Grover_rot_diag}.
        As the state is stable under the action of $\mathbf{Q}$, the rotaion angle of $\frac{\alpha}{2}$ is given. Thus, 
        Each application of $\mathbf{Q}$ rotates the state by $2\cdot \frac{\alpha}{2} = \alpha$, such that after $k$ iterations,
        \begin{equation*}
        \mathbf{Q}^k \ket{\psi} = \sin\left((2k+1)\frac{\alpha}{2} \right)\ket{\psi_g} + \cos\left((2k+1)\frac{\alpha}{2} \right)\ket{\psi_b}
        \end{equation*}
        
        The probability of measuring a good state thus increases quadratically with the number of iterations, reaching a maximum when $(2k+1)\frac{\alpha}{2} \approx \pi/2$,
        resulting in:
        \begin{equation}
            k = \lfloor \frac{\pi}{4}\sqrt{\frac{1}{P_g}} \rfloor
        \label{eq:needed_k}
        \end{equation}
        meaning, that after exactly $k$ rotations we will have a maximum amplification of our state. This repeated applications, each resulting in 
        a rotation with angle $\alpha$, can be seen in a 3-dimensional representation in figure~\ref{fig:Grover_rot_mine}, where the axes are the $bad$ ($\ket{\psi_b}$)
        and $good$ ($\ket{\psi_g}$) states, spanning $H_{\psi}$.

        \begin{figure}
        \centering
        \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/grover_rot.png}
        \caption{Grover rotation in the 2 dimensional subspace $H_{\psi}$}
        \label{fig:Grover_rot_diag}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/Quantum State Rotation 3D.png}
        \caption{ Repeated applications in the complete state space}
        \label{fig:Grover_rot_mine}
        \end{subfigure}
        %\caption{Amplitude amplification bz repeated Grover rotations}
        \label{fig:Grover_rot}
        \end{figure}


        \subsection{Scalability and Simulator Limitations}
        The scalability of amplitude amplification is determined by the complexity of the oracle and diffusion operators, as well as the 
        available quantum resources. For simple logical connectives and small models, both operators can be implemented with shallow 
        circuits, and the algorithm can be simulated efficiently on classical hardware. For more complex logical formulas or high-dimensional 
        models, the circuit depth and qubit count increase, and classical simulation becomes intractable.

        In this work, quantum simulations were performed for circuits up to 25 qubits using the Qiskit Aer simulator, balancing accuracy and 
        computational feasibility. For larger problem sizes, efficient simulation may require tensor network-based methods or access to real 
        quantum hardware. The quadratic speedup of amplitude amplification remains, but practical implementation is constrained by current 
        hardware and software limitations.
    
    \section{From Tensor Networks to Quantum Circuits}
    \label{sect:TN2Quantum}
        Tensor networks unify the representation of probabilistic models with quantum state encoding, as probabilistic models can be represented as such~\ref{subsect:Foundations_TN_ProbNetw},
        while quantum states - for example matrix product states (MPS) and projected entangled pair states (PEPS) - are inherently represented as tensor networks~\cite{TNforQC}.
        Therefore tensor networks provide a unified framework for representing and manipulating probabilistic models, as both probabilistic algorithms (e.g.inference),
        and quantum algorithms (such as Amplitude amplification) operate on these networks, providing a pathway to quantum-enhanced sampling.


            
        \subsection{Quantum Circuit Construction}
        \label{sect:TN2Quantum_construction}
            To map a tensor network to a quantum circuit, we need to map the very intricate structure, involving connections and dependencies precisely.
            Slice tensor decomposition~\ref{subsect:Foundations_EXP2TN_slice} helps us decompose the given network to lower-rank tensors (1 and 2, i.e. 
            vectors and matrices representing state vectors and density operators), that can be mapped to quantum states and operations easily. 
            Thus each tensor (i.e. slice) can be encoded as a quantum state and/or a set of parameterized 
            quantum gates. The contraction of tensors, which in the classical setting corresponds to summing over shared indices, is implemented 
            in the quantum setting by entangling qubits or applying multi-qubit gates. For example, in a network where each variable $X_i$ 
            is binary, a qubit is allocated for each variable, and the entries of the tensor slices determine the amplitudes of basis 
            states, which can be applied as parameterized gate-sets.
        \textbf{Operational Steps:}

        \textbf{Decomposition:}

            The joint probability tensor $\mathcal{T}[X_1, \ldots, X_n]$ for an $n$-variable exponential family distribution can be factorized as per~\ref{subsect:Foundations_EXP2TN_slice}: 
            \begin{align*}
                \mathcal{T}[X_1, \ldots, X_n] &= \sum_{k=1}^r \lambda_k \, \mathcal{A}_k[X_1] \otimes \mathcal{B}_k[X_2, \ldots, X_n]\\
                \intertext{or, more generally, as per~\ref{subsect:StepByStep}:}   
                \mathcal{T}[X_1, \ldots, X_n] &= \sum_{k=1}^r \mathcal{S}_k[X_1] \cdot \mathcal{U}_k[X_2] \cdots \mathcal{V}_k[X_n]
            \end{align*}
            where we have the tensor network uniqely decomposed into slices, ready to be represented as quantum states.

            This decomposition—closely related to the canonical polyadic (CP) decomposition, it maps naturally to quantum resources: 
            each vector or rank-1 slice defines amplitude weights for a subset of qubits, and higher-order correlations are encoded as 
            entanglement patterns or controlled gates.

        \textbf{Implementation:}
            Integrating the above with the earlier sections, the practical quantum circuit mapping unfolds as:
            \begin{itemize}
                \item \textbf{State Preparation}: 
                    \begin{itemize}
                        \item Prepare the quantum register in a reference state (e.g., $\ket{0}^{\otimes n}$).
                        \item Apply Hadamard gates to all variable qubits, creating a uniform superposition over all $2^n$ states.
                        \item Use CNOT, Toffoli, and (optionally) parameterized rotation gates to encode the logical structure specified 
                        by the tensor network slices, projecting logical connectives to designated ancilla qubits.
                    \end{itemize}
                \item \textbf{Oracle Construction}:
                    \begin{itemize}
                        \item Construct an oracle $O_\phi$ that flips the phase of those basis states corresponding to solutions (the "good" states satisfying $\phi$). 
                        \item In practice, for logical connectives, this can be realized by targeted $Z$, $X$, or multi-controlled $Z$ gates operating on the 
                        ancilla qubit(s) where the satisfaction of $\phi$ has been projected.
                    \end{itemize}
                \item \textbf{Diffusion Operator}:
                    \begin{itemize}
                        \item Apply the diffusion operator $D$, which inverts the state about the mean. This is constructed as $\mathcal{A}^{-1}$ (or $\mathcal{A}^+$), a phase flip on $\ket{0}^{\otimes n}$, and $\mathcal{A}$ again.
                        \item Both $\mathcal{A}$ and its inverse are efficient to implement due to the decomposition structure, mirroring the original tensor slice encoding.
                    \end{itemize}
                \item \textbf{Amplitude Amplification}:
                    \begin{itemize}
                        \item Iteratively apply $Q = D O_\phi$ to amplify the amplitude of the "good" state subspace.
                        \item The number of iterations $k$ is chosen according to the initial probability and the target amplification, as detailed in Section~\ref{subsect:QSA_AA_paramupdate}.
                    \end{itemize}
                \item \textbf{Measurement:}
                    \begin{itemize}
                        \item Measure the final state in the computational basis.
                        \item Measure the variable qubits (i.e. the nodes of our tensor network) in the computational basis.
                    \end{itemize} 
            \end{itemize}
            The pseudocode for each part of the procedure can be seen in \textcolor{magenta}{reference appendix for pseudocode}.


            By combining the slice decomposition approach (Section~\ref{subsect:Foundations_EXP2TN_slice}) with tensor-to-circuit 
            compilation presented in this section, this quantum circuit implementation provides a systematic bridge from classical 
            probabilistic and logical models to quantum-enhanced sampling routines. Each step - from state preparation using decomposed 
            tensor slices, to phase-marked oracles, to efficient inversion about the mean—follows directly from the underlying structure 
            revealed by the slice tensor decomposition. This guarantees scalability and resource efficiency for a wide range of complex probabilistic models.

            \begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black]
                Therefore utilizing the slice tensor decomposition we can map the example Bernoulli distribution to a quantum circuit easily.
                Each slice being jointly independent would mean that, we can not only encode them one-by-one, we do not even need to consider entangling gates/procedures.
                So for the distribution:
                \begin{equation*}
                P(x_1, x_2, \dots , x_n) = \prod_{i = 1}^n p_i^{x_i}(1-p_i)^{1-x_i}
                \end{equation*}
                we can encode each slice $p_i^{x_i}(1-p_i)^{1-x_i}$ to a quantum circuit via a sequence of gates, such as:
                \begin{itemize}
                    \item Generate a register for each variable, 1 for each slice.
                    \item Apply Hadamard gate $\hat{\textbf{H}}$ to create superposition.
                    \item Apply rotation gate around the $X$ or $Y$ axes ($\hat{\mathbf{R}}_X$ or $\hat{\mathbf{R}}_Y$) to encode the probabilities of the distribution resulting in $1$ or $0$, 
                    in the amplitude of the quantum state, i.e. mirroring the state resulting in $\ket{1}$ or $\ket{0}$ upon measurement.
                \end{itemize}
                Thus when we measure the circuit it should obtain one of the states representing the result of the measurement ($\ket{1}$ or $\ket{0}$), each with a probability
                that represents the distribution itself, obtained through the amplitude of the state. Also with a rotation applied to a superposition the state
                remains pure, so the probabilities will sum up to one.
                \\
                The amplitude amplification can be done for each slice via an easily applicable oracle and diffusion operator:
                \begin{itemize}
                    \item as we are working around binary variables a single \textbf{Z} or \textbf{XZX} gate(set) would suffice to mark the 
                    $\ket{1}$ and $\ket{0}$ states respectively,
                    \item the diffusion opreator should consist of the invers state preparation unitary $\mathcal{A}^{-1}$, and a single \textbf{X}
                    gate to rotate around the mean amplitude of our starting state (esentially flipping $\ket{0}$ to $\ket{1}$).
                    \item Finally $\mathcal{A} = \hat{\textbf{H}}\hat{\mathbf{R}}_X$ could be applied again, and we will get an amplified probability 
                    of obtaining $\ket{0}$ or $\ket{1}$ (depending on the Oracle).
                \end{itemize}
            \end{tcolorbox}

        \subsection{Resource Estimates}
            The quantum circuit depth and resource requirements depend on the structure and rank of the underlying tensor network:
            \begin{itemize}
                \item \textbf{Qubit Count}: Each variable in the model typically requires one qubit for binary variables (or 
                $\lceil \log_2 d_i \rceil$ qubits for $d_i$-ary variables) as well as one auxiliary qubit (i.e. ancilla qubit) for connecting the 
                cliques of probabilistic model representations~\ref{subsect:Foundations_TN_ProbNetw}.
                \item \textbf{Gate Complexity}: The number of gates scales with the number of tensor slices and the connectivity of the network. 
                For a CP decomposition of rank $r$ over $n$ variables, the circuit requires $\mathcal{O}(rn)$ parameterized gates~\cite{CPdecomp}.
                \item \textbf{Circuit Depth}: For chain-like (MPS) or tree-like (TTN) tensor networks, the depth is $\mathcal{O}(n)$ or $\mathcal{O}(\log n)$, respectively.
                %\item \textbf{Noise Considerations}: Shallow circuits and sparse connectivity are advantageous for near-term quantum devices, as they minimize decoherence and gate errors.
            \end{itemize}

            \begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black]
                As we have seen in the example mapping above we did:
                \begin{itemize}
                    \item require 1 qubit for each binary variable ($X_k \in \{X_1, X_2, \dots, X_n\}$),
                    \item each slice required 1 rotation gate for the probability encodings,
                    \item and each slice requires a constant number of gates for the amplitude amplification procedure $k \cdot c$.
                    \item So for n variables, we would need n variable qubits and 1 rotation gate per qubit, so n rotation gates, for the state preparation.
                    Also the amplification procedure would require $k \cdot c \cdot n$ gates in total.
                    Therefore the number of gates scale with the number of tensor slices, in the case of the example, with joint independence between the variables, 
                    it does scale linearly, resulting in a depth of $\mathcal{O}((2 + kc) \cdot n) \rightarrow \mathcal{O}(n)$ (constant multipliers are deductable).
                \end{itemize}
            \end{tcolorbox}
            We can see that the mapping is particularly efficient when the tensor network has low rank or sparse structure, as is often the case for 
            models with strong conditional independence. In such scenarios, the number of required gates and the circuit depth can be kept 
            polynomial in the number of variables, making the approach feasible for near-term quantum devices~\cite{Ran_2020}.

            In summary, the mapping from exponential family distributions to tensor networks, and subsequently to quantum circuits, enables scalable quantum sampling for complex probabilistic models, 
            as seen by the application of amplitude amplification later on. This approach exploits both the structure of the underlying model and the computational power of quantum devices, providing 
            a pathway to quantum advantage in probabilistic inference and machine learning.

%\section{Noise and Error Modeling}

%\subsection{Effect of Noise on Quantum Sampling}

%Quantum circuits are inherently sensitive to noise and decoherence, which can degrade the fidelity of amplitude amplification and reduce the probability of successfully measuring a good state. Common noise sources include gate errors, measurement errors, and qubit decoherence. Simulations incorporating realistic noise models show that amplitude amplification remains effective up to certain noise thresholds, but error accumulation can significantly impact performance for deeper circuits or larger systems. Understanding these effects is crucial for designing error-resilient circuits and for resource estimation on near-term quantum devices.


\chapter{Experimental Evaluation}

    \section{Use Case: Logical Connective Sampling}

        \subsection{Problem Setup}
        \label{subsect:EE_Setup}
            In this section, we consider a representative logical connective as the target for quantum sampling. The problem is defined as 
            follows: given a logical formula \textbf{F} (e.g., a conjunction or disjunction of $n$ Boolean variables), our goal is to efficiently 
            sample satisfying assignments (`good' states) from the exponentially large ($2^n$) space of all possible assignments.
            For concreteness, let us focus on a n-qubit system encoding a logical connective such as seen in the previous example~\ref{eq:Logical_conn_ex}, 
            which for the sake of the example will be 6 variables (mapped as qubits), with 19 connectives (ancillas). The set 
            of `good' states, $\mathcal{G}$, consists of all assignments that satisfy the formula. The initial probability of selecting a good 
            state from the uniform distribution is $P_g = M/N = |\mathcal{G}|/2^n$, in the case of my example shown, exactly $P_g = M/N = 1/2^6$.
            Our aim is to amplify $P_g$ to near-unity using quantum amplitude amplification.

        \subsection{Exponential family representation of the Logical Formula}
        \label{subsect:EE_expfam_repr}
            The logical formula \textbf{F} can be encoded as an exponential family distribution as per~\ref{subsect:Foundations_ExpFam_mapping}:
            \begin{equation}
                p(x) = \frac{1}{Z} \exp\left\{ \theta \cdot \hat{\mathrm{1}}[\textbf{F}] \right\}
                \label{eq:log_exp_fam}
            \end{equation}
            where $\hat{\mathrm{1}}[\textbf{F}]$ is the indicator function for the formula, serving as the sufficient statistics for the 
            exponential family, whilst $\theta$ is the canonical parameter, a scalar in our case, and $Z$ is the normalization constant.
            \\
            I have defined the weight added to the formula be:
            \begin{itemize}
                \item unit, whenever the indicator function gives a result of `FALSE'/0
                \item $e^{\theta}$, whenever it is `TRUE'/1
            \end{itemize}

            Using this representation, $\theta$ can be understood as the weight of the truth value of the formulas, coming directly from the sufficient 
            statistics being the indicator function of the logical formula, and therefore the weights being:
            \begin{itemize}
                \item $p(x = 1) = \frac{Me^{\theta}}{(N - M) + Me^{\theta}} = P_g$ 
                \item $p(x = 0) = \frac{N-M}{(N - M) + Me^{\theta}} = P_b$
            \end{itemize}
            with $N = 2^n$ as all the possible states, $M$ of which are `Good', i.e. results in the indicator function giving value of 1.
            This way it gives us a great starting point as for $\theta=0$, this reduces to the uniform distribution, and as $\theta \to \infty$, 
            the distribution concentrates on the satisfying assignments. Thus our expectation is solely this, that when the locical formula is mapped
            to a quantum circuit, we start from a uniform distribution of variables, and slowly amplify the probability of the `Good' states,
            only the canonical parameter of the formula will change, as it will diverge to infinite values, whilst the sufficient statistics should remain
            the same, esentially keeping us in the same exponential family with finely tuned variables, to match our amplified quantum states.
            \\
            Then we need to apply slice decomposition on the defined exponential family to map each intricate connection to the quantum circuit at hand.
            Following the section~\ref{subsect:Foundations_EXP2TN_slice} we can slice the logical formula by each connective, thus making it straightforward
            to iteratively have every detail embedded into our initial state $\ket{\psi}$, prepared by the unitary $\mathcal{A}$, the direct result of the decomposition.

        \subsection{Amplitude Amplification for Exponential Families}
        Our expectation is that only the canonical parameter of the formula will change, as it will diverge to infinite values, whilst the sufficient statistics should remain
        the same, esentially keeping us in the same exponential family with finely tuned variables, to match our amplified quantum states.
        As when the locical formula is mapped to a quantum circuit, we start from a uniform distribution of variables, and slowly amplify 
        the probability of the `Good' states, thus changing the probability of finding such.
        
        Mathematically, if the initial probability of a good state is $P_{g0}$, then after $k$ applications of amplitude amplification, the probability becomes $P_k = \sin^2\left((2k+1)\frac{\alpha}{2}\right)$, 
        where $\frac{\alpha}{2} = \arcsin(\sqrt{P_{g0}})$, is the angle of the state in $H_{\psi}$. In exponential family terms, this corresponds to a shift in the canonical parameter:
        \begin{align}
            \theta_k &= \ln\left(\frac{(1-P_{g0})P_k}{P_{g0}(1-P_k)}\right) \\
            \intertext{which can be rewritten as:}
            \theta_k &= \ln \left( \frac{P_k}{1 - P_k} \right) - \ln \left( \frac{P_{g0}}{1 - P_{g0}} \right)
            \label{eq:Can_par_change}
        \end{align}
        So $\theta_k$ is just the difference of log-odds (i.e. \textit{logit}, the inverse of the \textit{sigmoid}) between the current probability $P_k $and the initial probability $P_{g0}$.

        This provides a direct link between quantum amplitude amplification and classical parameter updates in probabilistic models.

            \iffalse
            \begin{figure}[h]
                \centering
                \includegraphics[width=0.7\textwidth]{quantum_circuit_example.pdf}
                \caption{Quantum circuit implementing amplitude amplification for a 6-qubit logical connective. The oracle marks `good' 
                states, and the diffusion operator amplifies their amplitudes.}
                \label{fig:aa_circuit}
            \end{figure}
            \fi

        \subsection{Quantum Circuit Implementation}
            The mapping from an exponential family model to a quantum circuit proceeds via the slice decomposition of its tensor 
            network representation. This operational workflow links the mathematical structure of probabilistic graphical models 
            directly to the physical implementation of quantum sampling algorithms. The explicit role of slice tensor decomposition 
            is to enable a direct, scalable quantum encoding of the high-dimensional probability distribution, efficiently reflecting 
            its conditional independence structure and logical symmetries.

            In this section, I am illustrating how a specific logical formula is mapped to a quantum circuit using the slice tensor 
            decomposition of its exponential family representation.

            Given a logical formula \textbf{F} first defined in sect~\ref{subsect:EE_Setup}, and represented as an exponential family distribution in sect.~\ref{subsect:EE_expfam_repr}:
            \begin{equation*}
                p(x) = \frac{1}{Z} \exp\left\{ \theta \cdot \hat{\mathrm{1}}[\textbf{F}] \right\}
            \end{equation*}
            where, just like in eq.~\ref{eq:log_exp_fam}, $\hat{\mathrm{1}}[\textbf{F}]$ is the indicator function for the formula \textbf{F} - i.e. the sufficient statistics of the 
            exponential family, whilst $\theta$ is the canonical parameter, and $Z$ is the normalization constant.
            
        To map this construction to a quantum circuit following sect.~\ref{sect:TN2Quantum_construction}:
        \begin{enumerate}
            \item \textbf{Slice Tensor Decomposition}: The indicator function $\hat{\mathrm{1}}[\textbf{F}]$ naturally partitions the joint tensor representing the distribution into two 
            slices—one for satisfying ("good") and one for non-satisfying ("bad") assignments. Mathematically, the joint distribution tensor is decomposed as:
            \[
            T[x] = \lambda_1 \cdot \hat{\mathrm{1}}[\textbf{F} = 1] + \lambda_0 \cdot \hat{\mathrm{1}}[\textbf{F} = 0]
            \]
            where the coefficients $\lambda_1 = e^{\theta}$ and $\lambda_0 = 1$ reflect the weights in the exponential family, just like I have defined in sect.~\ref{subsect:EE_expfam_repr}.
            \item \textbf{Quantum State Preparation}: The logical formula is compiled into a quantum circuit using ancilla qubits. Apply Hadamard gates to all variable qubits to create a 
            uniform superposition over $2^n$ possible inputs, then use Toffoli and controlled gates to compute the value of $F(x)$ onto an ancilla qubit.
            \item \textbf{Oracle Marking}: A $Z$ gate (phase flip) is applied to the ancilla qubit, conditionally marking the satisfying assignments. 
            This realizes the "slice selection" from the tensor decomposition within the quantum circuit.
            \item \textbf{Diffusion and Amplitude Amplification}: The standard amplitude amplification procedure is then applied, increasing the probability amplitude on the "good" slice. 
            After the desired number of amplification rounds, measurement yields satisfying assignments with high probability, while the underlying tensor slice structure ensures the 
            exponential family is preserved with an updated parameter $\theta$.
        \end{enumerate}

        In summary, this process implements the slice decomposition of the exponential family for a logical formula as a natural quantum circuit: logical structure—encoded as tensor 
        slices—is mapped onto programmable quantum operations and oracles, efficiently targeting the satisfying assignments for sampling and inference.



        \subsection{Results and Analysis}
            After $k$ rounds of amplitude amplification (so one additional turn for overrotation after near perfect amplification), the 
            probability of measuring a good state increases from $P_0$ to $P_k = \sin^2((2k+1)\alpha)$, 
            where $\alpha = \arcsin(\sqrt{P_0})$. The canonical parameter of the exponential family updates as seen in eq~\ref{eq:Can_par_change}.

            The said changes in probabilities on the given logical formula can be seen in the following figures:

            \begin{figure}[H]
            \centering

            \begin{subfigure}{\textwidth}
            \includegraphics[width=\linewidth]{pics/overrotation_final6.png}
            \caption{Changes in the probability of measuring TRUE value on the logical connective}
            \label{fig:AA_final}
            \end{subfigure}

            \medskip % insert a bit of vertical whitespace
            \begin{subfigure}{\textwidth}
            \includegraphics[width=\linewidth]{pics/overrotation_variable6.png}
            \caption{Changes in the input values resulting in given $P_{\mathcal{G}}$}
            \label{fig:AA_variable}
            \end{subfigure}

            \caption{Series of Grover rotations on a given logical formula}
            \label{fig:AA_log_formula}

            \end{figure}

            We can see that we have achieved near perfect amplification after 6 steps, which is in line with our mathematical framework, given in~\ref{subsect:QSA_AA_paramupdate},
            that $k = \lfloor \frac{\pi}{4}\sqrt{\frac{1}{P_g}} \rfloor = \lfloor \frac{\pi}{4}\sqrt{\frac{64}{1}} \rfloor = 6$. The fifth step, as expected,
            results in a lower probability, meaning that we have overrotated.

            The changes in angle and canonical parameters are:
            \begin{figure}[H]
            \centering

            \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\linewidth]{pics/can_prob.png}
            \label{fig:can_PROB}
            \end{subfigure}

            \medskip % insert a bit of vertical whitespace
            \begin{subfigure}{0.75\textwidth}
            \includegraphics[width=\linewidth]{pics/can_angle.png}
            \label{fig:can_ANG}
            \end{subfigure}

            \caption{Series of Grover rotations on a given logical formula}
            \label{fig:can_changes}

            \end{figure}

            Where we can see the expected shootout in the canonical parameters of the exponential family. The log-odds function, with which we define $\theta_k$,
            has vertical asymptotes at $P = 0$ and $P = 1$, with the added change in behaviour coming from the non-monotonic, but sinusoidal change of $P_k$.
            This understates those explosive `jumps', as the function reacts extremely to values close to 0 or 1, which shows us that with near perfect amplification,
            when $P_{\mathcal{G}} \rightarrow 1$, we have a local maxima in the canonical parameter.
            Furthermore, this shows that the sampling procedure remains within the same exponential family, but with an updated parameter $\theta_k$ reflecting 
            the amplified probability. The updated $\theta_k$ can be enumerated as:

            \begin{table}[h]
                \centering
                \begin{tabular}{|c|c|c|}
                    \hline
                    Iteration ($k$) & $P_k$ & $\theta_k$ \\
                    \hline
                    0 & $0.016$ & $0$ \\
                    \hline
                    1 & $0.134$ & $1.543$ \\
                    \hline
                    2 & $0.344$ & $2.763$ \\
                    \hline
                    3 & $0.592$ & $3.781$ \\
                    \hline
                    4 & $0.817$ & $4.905$ \\
                    \hline
                    5 & $0.963$ & $6.668$ \\
                    \hline
                    6 & $0.997$ & $9.215$ \\
                    \hline
                    7 & $0.908$ & $5.698$ \\
                    \hline
                \end{tabular}
                \caption{Amplification of probability and exponential family parameter over iterations.}
                \label{tab:amplification_results}
            \end{table}

            \noindent
            \textbf{Interpretation:} The amplitude amplification procedure efficiently concentrates probability mass on the set of 
            satisfying assignments, achieving quadratic speedup over classical rejection sampling. Importantly, the process preserves the 
            exponential family structure of the distribution, with the canonical parameter $\theta$ evolving according to the 
            amplification dynamics. This demonstrates the compatibility of quantum amplitude amplification with classical probabilistic 
            modeling frameworks.

    \section{Generalization and Scalability}
    \iffalse
    The approach generalizes to larger logical connectives and higher-dimensional systems. For $n$-qubit systems with more complex formulas, 
    the mapping to exponential families and tensor networks remains valid, and the quantum circuit construction follows the same principles. 
    Scalability is determined by the complexity of the oracle and the available quantum resources. Simulations up to 25 qubits demonstrate 
    robust amplification, with noise and circuit depth being the primary limiting factors for near-term quantum hardware.
    \fi

    \textcolor{red}{Put the more dimensional problem here}
    
\chapter{Benchmarking and Comparative Analysis}

    \section{Classical Sampling Methods}
        Sampling from complex probability distributions is a foundational task in statistics and machine learning, enabling probabilistic 
        inference, uncertainty quantification, and model training~\ref{sect:Intro_Challanges}. Classical sampling methods primarily include the brute-force approach,
        rejection sampling and Markov Chain Monte Carlo (MCMC) methods, each with distinct strengths and limitations~\cite{Ghojogh2020}.
        
        \subsection{Brute-Force Enumeration}
            Brute force sampling, also known as exhaustive enumeration, is the most direct approach to sampling from a probability distribution: 
            it systematically generates and evaluates every possible state in the sample space. For a discrete distribution over $N$ states, this 
            means explicitly listing all $N$ configurations, computing their probabilities, and either selecting samples according to their 
            weights or simply iterating through all possibilities.

            While conceptually simple and guaranteed to be exact, brute force methods are only feasible for very small systems. The time 
            and memory complexity scale as $\mathcal{O}(N)$, where $N$ is the total number of possible states. For problems involving $n$ 
            binary variables, $N = 2^n$, so the cost grows exponentially with $n$—a classic example of the `curse of dimensionality' or 
            combinatorial explosion, resulting in our case $\mathcal{O}(2^n)$~\cite{GFGBruteForce}.

            Brute force enumeration is sometimes used as a baseline for validating other sampling algorithms or for very small models. 
            However, for most real-world applications, the exponential scaling renders brute force approaches intractable. Even modest 
            increases in $n$ quickly make the approach impractical, motivating the use of more sophisticated methods such as rejection 
            sampling, MCMC, or quantum algorithms.

        \subsection{Rejection Sampling}
            Rejection sampling is a fundamental technique for generating samples from a target distribution $f(x)$ when direct sampling is 
            impractical. The method uses a proposal distribution $g(x)$, from which sampling is easy, and accepts or rejects each candidate 
            based on the ratio $f(x)/(M g(x))$, where $M$ is a constant such that $M \geq \sup_x p(x)/q(x)$ for all $x$. The efficiency of 
            rejection sampling heavily depends on the choice of the proposal distribution; if $g(x)$ poorly approximates $f(x)$, the 
            acceptance rate drops and the method becomes computationally inefficient~\cite{Lee2025}. The coice of said proposal distribution will define the $M$
            value, from which a suitable acceptance rate can be deduced. Despite its simplicity, rejection sampling can be wasteful for 
            high-dimensional or rare-event scenarios, as the expected number of trials to obtain one valid sample scales as $\mathcal{O}(M)$. 
            In rare events, where $P_g$ -- the probability of a `good' state -- is low, matched with a proposal distribution resulting in a 
            high M value $\approx \frac{1}{P_g}$, resulting in a computational complexity of $\mathcal{O}(\frac{1}{P_g})$, which just like with brute-force 
            algorithms, is becoming prohibitive for rare events ($P_g \ll 1$). With an easy-to-sample distribution, or a suitable proposal one, the value for 
            M will be much lower, and the acceptance rate higher. This means in the end a well built rejection sampling can achieve over the previously 
            defined complexity, albeit only by a constant multiplier. 

        \subsection{Markov Chain Monte Carlo (MCMC)}
            MCMC methods, such as the Metropolis-Hastings algorithm and Gibbs sampling, construct a Markov chain whose stationary distribution 
            is the target distribution~\cite{PMC5862921}. By sequentially generating correlated samples, MCMC can explore complex, 
            high-dimensional distributions even when direct sampling or rejection sampling is infeasible. The key advantage of MCMC is its 
            flexibility: it only requires the ability to compute the (unnormalized) density of the target distribution. However, MCMC methods 
            can suffer from slow mixing, especially in multimodal or high-dimensional spaces, and require careful tuning to ensure convergence 
            and independence of samples.
            \\
            MCMC constructs a Markov chain converging to $p(x)$. For Gibbs sampling, with $N$ being the number of possible states:
            \begin{itemize}
                \item Mixing time scales as $\mathcal{O}(e^{N})$ for multimodal distributions
                \item Per-iteration cost: $\mathcal{O}(N)$ for local updates
                \item Suffers from slow mixing in high dimensions due to metastability
            \end{itemize}
            So while classical sampling methods such as brute-force methods, rejection sampling and MCMC are widely used and well-understood, neither method 
            achieves better than linear scaling in $1/P_g$ or exponential in $N$, thus they face significant challenges in the context 
            of high-dimensional, structured, or rare-event distributions.

    \section{Quantum vs. Classical: Asymptotic Speedup}
        Quantum amplitude amplification achieves a quadratic improvement, requiring only $\mathcal{O}(1/\sqrt{p_g})$ queries. This follows 
        from Grover's theorem and its generalization to amplitude amplification:
        \begin{theorem}[Brassard et al. 2000]
        Given an initial state $A|0\rangle$ with good-state probability $p_g = \sin^2\alpha$, after $k = \lfloor \pi/(4\alpha) \rfloor$ 
        iterations of $Q = -AS_0A^\dagger S_\chi$, the probability of measuring a good state satisfies:
        \begin{equation}
        P_{\text{good}} = \sin^2((2k+1)\alpha) \geq 1 - p_g
        \end{equation}
        \end{theorem}

        \begin{proof}
        The state evolution under $Q$ performs a rotation in a 2D subspace spanned by $|\psi_g\rangle$ and $|\psi_b\rangle$~\ref{subsect:QSA_AA_paramupdate}. 
        After $k$ iterations:
        \begin{equation*}
        Q^k A|0\rangle = \sin((2k+1)\alpha)|\psi_g\rangle + \cos((2k+1)\alpha)|\psi_b\rangle
        \end{equation*}
        Choosing $k = \lfloor \pi/(4\alpha) \rfloor$ yields $(2k+1)\alpha \in [\pi/2 - \alpha, \pi/2 + \alpha]$, giving $P_{\text{good}} \geq \cos^2\alpha = 1 - p_g$.
        \end{proof}

        This demonstrates that $\mathcal{O}(1/\sqrt{p_g})$ iterations suffice to amplify the success probability near 1, quadratically 
        faster than classical rejection sampling.
        \\
        \textbf{Full Circuit Depth Analysis}
            Although we see that the amplitude amplification achieves a quadratic improvement over classical algorithms, it is only the
            'search' part of our procedure, and the quantum advantage must account for full circuit depth, not just oracle queries. 
            Thus we need to include the state preparation unitaries as well:
            \begin{itemize}
                \item $D_A$: Depth of state preparation circuit
                \item $D_O$: Depth of oracle implementation
                \item $D_D$: Depth of diffusion operator
            \end{itemize}
            The total depth for $k$ iterations is:
            \begin{equation}
            D_{\text{total}} = D_A + k(D_O + D_D)
            \end{equation}

            Which in my implementation sums up to:
            \begin{itemize}
                \item Uniform preparation: $D_A = \mathcal{O}(n)$ (Hadamard gates + NOT/Toffoli gates for logical formula mapping)
                \item Logical oracle: $D_O = \mathcal{O}(1)$ (utilizing the symmetry between $D_A$ and $D_D$)
                \item Diffusion: $D_D = \mathcal{O}(n)$ (essentially overlapping with the state preparation unitary)
            \end{itemize}
            
            \iffalse
            \textcolor{magenta}{open up iffalse when I write the extensions for generic applications not only logical formulas}
            and for typical implementations sums up to:
            \begin{itemize}
                \item Uniform preparation: $D_A = \mathcal{O}(1)$ (Hadamard gates)
                \item Logical oracle: $D_O = \mathcal{O}(n)$ (for $n$-variable connectives)
                \item Diffusion: $D_D = \mathcal{O}(n)$
            \end{itemize}
            \fi
            Thus $D_{\text{total}} = \mathcal{O}(n/\sqrt{p_g})$, still linear in variables, preserving quadratic speedup.


\section{Resource Estimation and Practical Feasibility}
The choice of quantum hardware platform is a critical factor in translating theoretical quantum speedup into practical, 
real-world performance. Different technologies offer distinct trade-offs in terms of gate speed, fidelity, connectivity,
and scalability, all of which directly affect the feasibility and efficiency of implementing quantum algorithms, such as 
the proposed sampling procedure.
    \subsection{Platform proposal}
    The selection of a quantum hardware platform should be guided by the specific requirements of the task at hand. For this 
    thesis, the focus is on sampling from complex logical connectives and exponential family distributions. When 
    evaluating platforms for such algorithms that -- with utilizing amplitude amplification, i.e. repeated Grover's algorithm -- 
    require deep circuits and many multi-qubit entangling gates, key considerations include:

    \begin{itemize}
        \item \textbf{Gate Fidelity and Error Rates:} High-fidelity gates are essential to maintain coherence and ensure the 
        reliability of sampling results, especially as circuit depth increases.
        \item \textbf{Coherence Times:} Longer coherence times allow for more complex algorithms to run without significant 
        decoherence, which is crucial for amplitude amplification and other iterative procedures.
        \item \textbf{Qubit Connectivity:} All-to-all connectivity enables direct entanglement between any pair of qubits, 
        reducing the need for SWAP gates and simplifying circuit compilation for highly non-local operations.
        \item \textbf{Native Gate Set:} The availability of native multi-qubit gates or efficient decomposition of logical 
        operations can greatly affect the overall circuit depth and execution time.
        \item \textbf{Scalability:} The ability to scale up to larger numbers of qubits without a significant drop in 
        performance is important for tackling high-dimensional sampling problems.
    \end{itemize}

    \vspace{1em}
    \noindent
    \textbf{Platform Selection Rationale}

    While superconducting qubit platforms offer fast gate times and are widely accessible, their limited qubit connectivity (nearest neighbour) 
    and shorter coherence times can become bottlenecks for deep or highly connected circuits. In contrast, trapped ion quantum computers 
    provide several compelling advantages for this class of problems:

    \begin{itemize}
        \item \textbf{All-to-All Connectivity:} Eliminates the need for SWAP gates, allowing direct implementation of multi-qubit 
        logical connectives and reducing compilation overhead.
        \item \textbf{Superior Gate and Measurement Fidelity:} Minimizes cumulative error, making it feasible to execute longer 
        circuits with high accuracy.
        \item \textbf{Native support for multi-qubit gates:} Some platforms implement multi-qubit entangling gates directly, 
        further reducing circuit depth for logical oracles and diffusion operators.
        \item \textbf{Exceptionally Long Coherence Times:} Supports deep amplitude amplification sequences and complex sampling 
        routines without significant loss of quantum information.
        \item \textbf{Scalability:} Recent advances have demonstrated the ability to operate with dozens of qubits, with roadmaps 
        to even larger systems.
    \end{itemize}

    So, although superconducting qubits have been the workhorse of many quantum computing experiments, trapped ion platforms are 
    particularly well-suited for quantum sampling tasks that involve high circuit depth and complex logical structure. 
    For these reasons, it is the platform, which poses as the most well-suited choice, while still being accessible enough, 
    many instances being provided by individual companies and research groups. The following table details the relevant 
    hardware specifications and performance characteristics of trapped ion quantum computers, providing a concrete foundation 
    for resource estimation and benchmarking of the proposed quantum sampling algorithms.





\subsection{Circuit Compilation and Scaling for $n$ Qubits}

For a general $n$-qubit logical connective (with $N = 2^n$ states and one `good' state, $p_g = 1/N$):

\begin{itemize}
    \item \textbf{Oracle Implementation:}
        \begin{itemize}
            \item Depth $D_O = \mathcal{O}(1)$ for simple connectives or $\mathcal{O}(n)$ for general multi-controlled gates, but no SWAP overhead due to all-to-all connectivity.
            \item Ancilla management: $\mathcal{O}(n)$ auxiliary qubits for intermediate results, as in other platforms.
        \end{itemize}
    \item \textbf{Diffusion Operator:}
        \begin{itemize}
            \item Depth $D_D = \mathcal{O}(1)$ for global operations, or $\mathcal{O}(n)$ for decomposed multi-controlled gates, again with no connectivity penalty.
        \end{itemize}
    \item \textbf{State Preparation:}
        \begin{itemize}
            \item Hadamard gates on all $n$ qubits (depth $D_A = 1$).
        \end{itemize}
    \item \textbf{Total resources per amplitude amplification iteration:}
        \[
        \text{Depth per iteration: } D_{\text{iter}} = D_A + D_O + D_D \approx 1 + 2n
        \]
        \[
        \text{Qubits: } n + \mathcal{O}(n) \text{ (data + ancilla)}
        \]
\end{itemize}

\noindent
\textbf{Transpilation Considerations:}  
Although some logical gates (e.g., Toffoli, multi-controlled $Z$) are not native to trapped ion hardware, all-to-all connectivity and high-fidelity two-qubit gates enable efficient transpilation. The absence of SWAPs and the possibility of direct multi-qubit entangling gates (e.g., Mølmer–Sørensen) mean that even transpiled circuits remain shallow compared to superconducting platforms for the same logical depth.

\subsection{Quantum vs. Classical Runtime Scaling}

For a target probability $p_g = 1/2^n$:

\begin{itemize}
    \item \textbf{Classical rejection sampling:}
        \[
        T_{\text{classical}} = \frac{1}{p_g} \cdot t_c = 2^n \cdot t_c
        \]
        where $t_c$ is the classical step time (e.g., $1$ ns).
    \item \textbf{Quantum amplitude amplification:}
        \[
        T_{\text{quantum}} = \frac{1}{\sqrt{p_g}} \cdot D_{\text{iter}} \cdot t_g = 2^{n/2} \cdot (1 + 2n) \cdot t_g
        \]
        where $t_g$ is the two-qubit gate time (e.g., $100$ $\mu$s).
\end{itemize}

\subsection{Turnover Point Analysis}

Quantum advantage is achieved when $T_{\text{quantum}} < T_{\text{classical}}$, or:
\[
2^{n/2} \cdot (1 + 2n) \cdot t_g < 2^n \cdot t_c
\]
Assuming $t_g = 100\ \mu$s and $t_c = 1$ ns, this simplifies to:
\[
10^5 \cdot (1 + 2n) < 2^{n/2}
\]

\textbf{Numerical solution:}
\begin{itemize}
    \item For $n=30$: $10^5 \times 61 = 6.1 \times 10^6 < 2^{15} = 32,768$ (not yet quantum-advantaged)
    \item For $n=40$: $10^5 \times 81 = 8.1 \times 10^6 < 2^{20} = 1,048,576$ (not yet quantum-advantaged)
    \item For $n=50$: $10^5 \times 101 = 1.01 \times 10^7 < 2^{25} = 33,554,432$ (quantum begins to win)
    \item For $n=60$: $10^5 \times 121 = 1.21 \times 10^7 < 2^{30} = 1,073,741,824$ (quantum is much faster)
\end{itemize}

\noindent
\textbf{Conclusion:} The quantum-classical turnover for rare-event sampling with $p_g = 2^{-n}$ occurs at $n \approx 50$ qubits on current trapped ion hardware, accounting for slower gate times but all-to-all connectivity and high fidelity. For structured problems or higher $p_g$, the turnover occurs at lower $n$.

\subsection{Performance Projections and Practical Implications}

\begin{table}[h]
\centering
\caption{Sampling performance for $n$-qubit system ($p_g=2^{-n}$, $t_g=100\ \mu$s)}
\begin{tabular}{l|c|c|c}
\textbf{Method} & \textbf{Time/sample} & \textbf{Samples/sec} & \textbf{Error} \\
\hline
Rejection (CPU) & $2^n$ ns & $2^{-n}$ ns$^{-1}$ & 0\% \\
Quantum (trapped ion HW) & $2^{n/2} \cdot (1+2n) \cdot 10^5$ ns & $[2^{n/2} \cdot (1+2n) \cdot 10^5]^{-1}$ ns$^{-1}$ & $<1\%$ \\
\end{tabular}
\end{table}

\noindent
\textbf{Sensitivity Analysis:}
\begin{itemize}
    \item If $t_g$ improves to $50\ \mu$s: turnover at $n=45$
    \item If $D_O$ and $D_D$ are optimized (e.g., via native multi-qubit gates): turnover at $n=40$
    \item For higher $p_g$ (more `good' states): turnover at lower $n$
\end{itemize}

\subsection{Error Mitigation Requirements and Outlook}

To achieve practical quantum advantage in sampling:
\begin{itemize}
    \item Two-qubit fidelities $>99.9\%$ (current: 99.9–99.99\%)
    \item Coherence times $>1$ min (current: $>10$ min already sufficient)
    \item Error mitigation (zero-noise extrapolation, dynamical decoupling, probabilistic error cancellation) can reduce sampling error to below $1\%$
    \item No need for SWAP gates or complex routing, thanks to all-to-all connectivity
\end{itemize}

\noindent
\textbf{Roadmap:}
\begin{itemize}
    \item 2026: 32–50 qubit demonstrations for rare-event sampling ($p_g \leq 10^{-10}$)
    \item 2027: Native multi-qubit gate integration and further gate speed improvements
    \item 2028: Commercial trapped ion quantum sampling coprocessors for scalable ML workflows and hybrid quantum-classical inferenc
\end{itemize}

\noindent
\textbf{Summary:} Trapped ion quantum hardware provides a compelling platform for quantum-enhanced sampling, particularly for deep circuits and rare-event problems. The combination of all-to-all connectivity, long coherence times, and ultra-high gate fidelities offsets the slower gate speeds compared to superconducting qubits. As a result, quantum advantage for sampling is projected to emerge at $n \geq 50$ qubits for unstructured search, and significantly earlier for structured or highly parallelizable logical connectives. These features make trapped ion systems especially attractive for scalable quantum machine learning and probabilistic inference in the near future.


\appendix
    %\chapter{Supplementary Material}
    \chapter{Code Listings}
    \label{Appendix_pseudocodes}
    %\chapter{Additional Figures and Tables}

\backmatter
\printbibliography

\end{document}