method development for sampling from markov networks / bayesian networks

turn the recieved problem into a quantum algorithm
explore the scalability

continue on Jaro path -- map QUBO to HUBO and solve them 
include the beginning statement of course to have a bag  in which we include the q alg.


DATEV meeting

sum up the exponential family representations
-sample from them
-reformulte the energy tensor as HUBO
-solve it
-why is it better than sampling from than the Graphical models

HUBO/QUBO solutions and the tweaking of the program

DONE
---------------------------dec 16.
Sampling programs

DATEV meeting 
represent the knowledge base as exponential families

sample from markov logic networks, by restricting to the hard formulas -- not to infer only the energies and calculate the highest but also the whole parameter space 
Markov logic networks with hard constraints == Bayesian networks with evidence

with markov networks we do not need any more evidence as we have hard constraints
we know the parameters so we need to do deductive reasoning -- deterministic computing as in sampling

generate a quantum circuit that encodes the logical knowledge base 

ESENTIALLY:
get knowledge bases/distributions
model them as tensor networks == exponnetial families
reshape them as quantum circuits, that encode the logic between the variables of the base data
sample from this quantum circuit with 

DONE
----------------------------jan 15

Have some method to convert the nested list into slices of lists

then we can map these to QCs

Also the QCs need to be optimized, because in the end we only measure one qubit,
and all the others can be dropped --> so it can be faster 

ADD ANCILLA FOR NOT GATES TOO

DONE

----------------------------jan 24

Check the ampluitude amplification better, qiskit does it fine!
I have an entangled state so we should have a backpropagation from biasing the result states to be 1! 


Check ALexs list decomposition and see how it defines the subformulas
encoding.get_formula_color(expression) 
tnreason labels tensors with colors, these are strings associated with each tensor -- shows us how to contract

DONE


------------------------------febr 9
We can say that the quantum rejection sampling is faster thatn the classical one because of the amplitude amnplification,
but classically we will not always use the rejection sampling, so we need to say that we have a speeup in some cases only

Also try to write something for quantum tensoir networks, logical connective mapping, measurements

DONE

------------------------------ febr.25
--Ampl. Aplif
Apply a post processing procedure to remove the states being created but not the states being "REAL" 
--> do a pen and paper calculation
Check diffusion operator for the Ampl. Amplif., but also the oracle, becuase it may mark states that i do not need!

also can we think of amplitude amplification in a way that we want to precisely influence teh resulting distributions 
--> maybe see ghow the prop. is shaped after each run
so to see how the amplitude amplification can be understod using probabilistic weights

--KB to QC
try to make a long random nested list generator to check (trial and error) of the automatized mapping precision and speed.
Try to make it as long as possible and see how long of a logical connective can be run under 10 secs

--> data encoding before the logical connectives and the amplification

--OR gates
check if we could go beyond binary connectives
see how the controlled gates are connected to slice decompositions --> 18.1.3 enexaproj.
try the polycore formatting of tnreason --> modulo 2 contractions by phase flips
and also see for the multi controlled gates and how do they work in qiskit
HOW TO PUSH IT TO MORE GENERIC TENSOR APPLICATIONS WHEN NO SYNTAX IS PROVIDED -- boolean tensor encoding and calculating by ancilla qubits

--Fianlly start wrting and collecting data/bibliography 
see the speedup of the procedure

------------------------------ mar. 05.

Generate a more generic approach for the Amplitude amplification
now we do grover iterations, but we need to see how each one works -- what do they apply -- dont just check the asymptotic behaviour of the algorithm but he state representation/applied weights
--slice tensor quantum circuit representation, do it as a second way of formalizing the tensors (thr first was the logical connective approach) from the polycore engine

so find the difference between Q hard logic and Q soft logic

-- we make  hard logic kb and then with the sampling we turn to soft logic, and by sampling we map to exp families.

Like in the MLN paper how do they prepare q states

---theoretical background
so now we have an experimental example for the project, using hard logic propositional KBs
the theoretical background could be of about exponential families representing various probability distribution / graph. models
these can be sampled using generic tensor network approaches what we show in our example now
and in the example we start from a hard logic distribution, but then with the Q. rejection sampling (amplitudeA.) we turn to soft logic
with the whole sampling procedure representing exponential families
